<!doctype html>
<html>
<head>
    <style>
        body {font-family: Arial, Helvetica, sans-serif;}
        
        #myImg {
          border-radius: 5px;
          cursor: pointer;
          transition: 0.3s;
        }
        
        #myImg:hover {opacity: 0.7;}
        
        /* The Modal (background) */
        .modal {
          display: none; /* Hidden by default */
          position: fixed; /* Stay in place */
          z-index: 1; /* Sit on top */
          padding-top: 100px; /* Location of the box */
          left: 0;
          top: 0;
          width: 100%; /* Full width */
          height: 100%; /* Full height */
          overflow: auto; /* Enable scroll if needed */
          background-color: rgb(0,0,0); /* Fallback color */
          background-color: rgba(0,0,0,0.9); /* Black w/ opacity */
        }
        
        /* Modal Content (image) */
        .modal-content {
          margin: auto;
          display: block;
          width: 80%;
          max-width: 700px;
        }
        
        /* Caption of Modal Image */
        #caption {
          margin: auto;
          display: block;
          width: 80%;
          max-width: 700px;
          text-align: center;
          color: #ccc;
          padding: 10px 0;
          height: 150px;
        }
        
        /* Add Animation */
        .modal-content, #caption {  
          -webkit-animation-name: zoom;
          -webkit-animation-duration: 0.6s;
          animation-name: zoom;
          animation-duration: 0.6s;
        }
        
        @-webkit-keyframes zoom {
          from {-webkit-transform:scale(0)} 
          to {-webkit-transform:scale(1)}
        }
        
        @keyframes zoom {
          from {transform:scale(0)} 
          to {transform:scale(1)}
        }
        
        /* The Close Button */
        .close {
          position: absolute;
          top: 15px;
          right: 35px;
          color: #f1f1f1;
          font-size: 40px;
          font-weight: bold;
          transition: 0.3s;
        }
        
        .close:hover,
        .close:focus {
          color: #bbb;
          text-decoration: none;
          cursor: pointer;
        }
        
        /* 100% Image Width on Smaller Screens */
        @media only screen and (max-width: 700px){
          .modal-content {
            width: 100%;
          }
        }
    </style>

   

    <meta charset='utf-8'>
    <meta http-equiv='x-ua-compatible' content='ie=edge'>
    <link rel="shortcut icon" href="images/formyreference.png" type="image/x-icon">
    <title>operating system for your reference</title>
</head>
<body>
<pre>
                                                                                                        <cite>ജയകുമാർ നായർ തയ്യാറാക്കിയത് 
                                                                                                        reference: 
                                                                                                        Operating System Concepts, Ninth Edition by Abraham Silberschatz, Peter Baer Galvin and Greg Gagne
                                                                                                        https://www.youtube.com/channel/UCVZD4ph7tqdAbVO4URH9QGw/videos (need to refer)
                                                                                                        https://www.youtube.com/watch?v=GryqH5TiNmc  (need to refer)
                                                                                                        https://youtu.be/wJBDIwU5m20   (need to refer)
                                                                                                                   </cite>
<hr>
<details>
    <summary id="id_somethingextra_detail">drafted part</summary>
<table border="0" width="100%">
    <tr>
        <td align="middle"><img src="images/0.os-birds-eye-view.jpg" alt="" width="35%"></td>
        <td>    
            <b><a href="#id_introduction_detail" style="color: rgb(0, 0, 0); text-decoration: none;">introduction</a></b>
            <b><a href="#id_operatingsystemstructures_detail" style="color: rgb(0, 0, 0); text-decoration: none;">operating-system structures</a></b>
            <b><a href="#id_processes_detail" style="color: rgb(0, 0, 0); text-decoration: none;">processes</a></b>
            <b><a href="#id_threads_detail" style="color: rgb(0, 0, 0); text-decoration: none;">threads</a></b>
            <b><a href="#id_processsynchronization_detail" style="color: rgb(0, 0, 0); text-decoration: none;">process synchronization</a></b>
            <b><a href="#id_cpuscheduling_detail" style="color: rgb(0, 0, 0); text-decoration: none;">cpu scheduling</a></b>
            <b><a href="#id_deadlocks_detail" style="color: rgb(0, 0, 0); text-decoration: none;">deadlocks</a></b>
            <b><a href="#id_mainmemory_detail" style="color: rgb(0, 0, 0); text-decoration: none;">main memory</a></b>
            <b><a href="#id_virtualmemory_detail" style="color: rgb(0, 0, 0); text-decoration: none;">virtual memory</a></b>
            <b><a href="#id_massstoragestructure_detail" style="color: rgb(0, 0, 0); text-decoration: none;">mass-storage structure</a></b>
            <b><a href="#id_filesysteminterface_detail" style="color: rgb(0, 0, 0); text-decoration: none;">file-system interface</a></b>
            <b><a href="#id_filesystemimplementation_detail" style="color: rgb(0, 0, 0); text-decoration: none;">file-system implementation</a></b>
            <b><a href="#id_i/osystems_detail" style="color: rgb(0, 0, 0); text-decoration: none;">i/o systems</a></b>
            <b><a href="#id_protection_detail" style="color: rgb(0, 0, 0); text-decoration: none;">protection</a></b>
            <b><a href="#id_security_detail" style="color: rgb(0, 0, 0); text-decoration: none;">security</a></b>
            <b><a href="#id_virtualmachines_detail" style="color: rgb(0, 0, 0); text-decoration: none;">virtual machines</a></b>
            <b><a href="#id_distributedsystems_detail" style="color: rgb(0, 0, 0); text-decoration: none;">distributed systems</a></b>
        </td>
    </tr>
</table>
<hr>
<b id="id_introduction_detail">introduction</b>
# what operating systems do
    = a computer system can be divided roughly into four components
        - hardware
        - operating system
        - application programs
        - users
    = operating system manages computer hardware, system programs, application programs and users
    = an operating system acts as an intermediary between the user of a computer and the computer hardware
    = operating system is the program running at all times on the computer called the kernel
    <img src="images/1.1.kernel.jpg" alt="" width="15%" height="10%">
    = 2 other types of programs along with the kernel are
        - system programs (associated with the operating system but are not necessarily part of the kernel)
        - application programs (not associated with the operation of the system)
    = mobile operating systems often includes a middleware (set of software frameworks provides services to application developers)
# computer-system organization
    <img src="images/1.2.moderncomputersystem.jpg" alt="" width="20%" height="10%">
    = computer-system components
        - a modern computer system consists of one or more CPUs and a number of device controllers connected through a common bus that provide access to shared memory
        - each device controller is in charge of a specific type of device
        - CPUs and device controllers can execute in parallel competing for memory cycles
        - memory controller synchronizes access to the memory
    = computer-system starting
        - when a computer powered up or reboot the initial program to execute is bootstrap program
        - typically this program is stored with in the computer hardware in ROM - readonly memory or firmware(EEPROM - electronically erasable programmable memory)
        - the bootstrap program must locate the operating system kernel and load into memory
        - the kernel starts providing services to the system and the users, some services are provided by few external programs
        - these external programs are called system programs which is loaded to memory at boot time to become system process or daemons(in unix first daemon is 'init')
    = devices and CPU communication(events)
        - events are occurred through interrupt mechanism from either the hardware or the software
        - hardware may trigger an interrupt at any time by sending a signal to the CPUs usually by way of system bus
        - software may trigger an interrupt at any time by executing a special operation called a system call(monitor call)
        - when a CPU is interrupted, it stops the current operation and transfers to a fixed location
        - that fixed location contains the starting address of service routine for the interrupt
        - on completion, the CPU resumes the interrupted computation
        <img src="images/1.3.interrupttimeline.jpg" alt="" width="70%" height="40%">
    = storage structure
        <img src="images/1.4.storagehierarchy.jpg" alt="" width="20%" height="10%">
        - all forms of memory provide an array of bytes and every byte has an address
        - main memory mainly implemented in a semiconductor technology called DRAM-dynamic random access memory is called RAM
        - SSD, solid state drive is a non-volatile memory and stores data in a DRAM array during normal operation but also contains a hidden magnetic hard disk and a battery
        - when external power is interrupted the SSD's controller will copy data from RAM to disk so its becomes non-volatile. when the external power is restored controller copies the data back to RAM  
        - another non-volatile form is flash memory which doesn't require power to retain its contents and its a variant of EEPROM-electrically erasable programmable read-only memory 
        - another non-volatile form is NVRAM which is DRAM with battery power
    = i/o structure 
        <img src="images/1.5.1.devicecontrolleranddriver.jpg" alt="" width="17%" height="10%">        <img src="images/1.5.2.diffdevicecontrolleranddriver.jpg" alt="" width="30%" height="25%">
        - every operating system have a device driver for each device controller
        - device driver is a software program with which the device controller interacts with the operating system
        - device controller is a hardware unit attached to the I/O bus and works like an interface between a device and a device driver
        - device controller maintains some local buffer storage and a set of special purpose registers and it is responsible for moving the data between the peripheral devices and its local buffer storage 
        - each device controller is in charge of a specific type of device
        - SCSI-small computer systems interface controller can attach to seven or more devices 
        - how data movement happens (small amount of data movement)
            . Execution cycle finds a user input instruction
            . CPU stops the current user process execution and starting the I/O interrupt processing (identify the interrupt routine address from the interrupt vector)
            . interrupted device driver loads instructions to appropriate registers on the device controller
            . device controller examines the contents of these registers to determine what action to take(like read)
            . device controller starts the transfer of data from the I/O device to its local buffer
            . device controller informs the device driver via an interrupt that it has finished the operation
            . device driver then returns control to the operating system and load data to memory
        - how data movement happens (bulk data movement)
            . device controller populate local buffers and transfers bulk of data to and from directly to memory
            . this movement does not involve CPU and its called direct memory access(DMA)
# computer-system architecture
    = execution of instructions
        - Von Neumann architecture was first published by John von Neumann in 1945
        <img src="images/1.6.vonneumannarchitecture.jpg" alt="" width="17%" height="10%">
        - it is based on the stored-program computer concept, where instruction data and program data are stored in the same memory
        - interaction with CPU and memory unit is achieved through load and store instructions to specific memory addresses
        - load - from main memory to internal registers within the CPU
        - store - from internal registers to main memory
        - need arises, CPUs explicitly loads instructions from main memory for execution
        - instructions are processed via fetch-execute cycle
        - CPU can load instruction only from main memory(RAM-random access memory)
        <img src="images/1.7.fetchexecutecycle.jpg" alt="" width="20%" height="8%">
    = single processor systems
        - one main general purpose processor 
        - may have many special purpose processor(disk, keyboard and graphics controllers)
    = multi processor systems
        - 2 or more processors in close communication sharing the computer bus, clock memory and I/O devices also known as parallel systems or multicore systems
        - three main advantages are 
            . increased throughput, speed up ration if N processor is not N but near
            . economy of scale, less cost because they share many components(peripherals,storage and power supplies)
            . increased reliability, failure of one processor will not halt the system, only slow down thus controlling failing altogether
        - 2 types of multiprocessing systems are currently in use
            . asymmetric multiprocessing: boss-worker relationship, OS tasks are carried only by the master processor which will allocate work to the worker processor. Sun Microsystems SunOS version 4
            . symmetric multiprocessing(SMP): peer relationship, all the processors can take part in the OS tasks based on availability. Sun Microsystems SunOS version 5
            <img src="images/1.8.symmetricmultiprocessing.jpg" alt="" width="20%" height="8%">
            . differentiator for asymmetric/symmetric multiprocessing systems are either hardware or software. software can be written to allow only one boss and multiple workers
        - terms used for reliability of computer system
            . graceful degradation, ability to provide service proportional to the level of surviving hardware
            . fault tolerant, hardware and software duplication to ensure continued operation despite failure
        - multiprocessing can cause a system to change its memory access model
            . UMA-uniform memory access, access to any RAM from any CPU takes same amount of time
            . NUMA-non-uniform memory access, access to some part of RAM may takes long amount of time. OS can minimize the NUMA penalty through resource management
        - multicore, including multiple computing cores in a single chip
        <img src="images/1.9.multicore.jpg" alt="" width="15%" height="25%">
            . all multicore systems are multiprocessor systems
        - blade servers, multiple processor boards and I/O boards and networking boards are placed in the same chassis
            . each blade processor boards boots independently and runs its own OS
    = clustered systems
        - composed of multiple individual systems or nodes or loosely coupled systems
        - generally clustered computers share storage and are closely linked via LAN or a faster interconnect such as InfiniBand
        - a layer of cluster software runs on the cluster nodes
        - each nodes can monitor one or more of the others(over LAN)
        - if monitored machine fails, the monitoring machine can take ownership of its storage and restart the applications that were running on the failed machine
        - 2 types of clustered systems
            . asymmetric clustering, one hot-standby machine will monitor the active server and in case of failure it becomes the active server
            . symmetric clustering, 2 or more hosts are running applications and are monitoring each other
        - application must written to take advantage of the cluster, involving the technique known as parallelization
            . parallelization divides a program into separate components and run in parallel on individual computers in a cluster
            . once each node solves its portion, the results from all the nodes are combined into a final solution
        - other forms clusters are 
            . parallel clusters
            . clustering over a WAN-wide area network
        - parallel clusters allow multiple hosts to access the same data on shared storage
        - normal OS won't support simultaneous data access by multiple hosts a special software required for achieving the same. Oracle Real Application Cluster is designed to run on a parallel cluster
        - every machine runs Oracle and a layer of software tracks access to the shared disk
        - to avoid conflict on data DLM-distributed lock manager is included in cluster technology
        - SAN-storage area network technology allows many systems to attach to a pool of storage
        - cluster software can assign the application to run on any host that is attached to the SAN
        <img src="images/1.10.structureofaclusteredsystem.jpg" alt="" width="20%" height="25%">
# operating-system structure
    = OS provides an environment within which programs are executed
    = lots of jobs will kept initially on the disk in the job pool
    = job is a collection of program + input data + control instructions
    <img src="images/1.11.job.jpg" alt="" width="9%" height="13%">
    = operating system will bring subset of jobs from job pool to memory since memory is small to accommodate all the jobs 
    = OS picks and begins to execute one of the job
    = some of the types of operating systems
        - multiprogramming
        - multi tasking/time-sharing
        - batch
        - multiprocessing
        - real-time
    = what is multiprogramming
        - CPU will continue to execute the job until finishes (non-preemptive) or the job request for some I/O operation
        - main purpose of multiprogramming is to make sure that the CPU is always busy
        - CPU will have one job at a time
    = what is multitaksing or time-sharing
        - multitaksing or time-sharing is an extension on multiprogramming
        - every job will execute only for a predefined time or CPU scheduling
        - CPU will execute a job until time slice reaches (preemptive) or the job request for some I/O operation then start executing the next one 
        - main purpose of multitasking or time sharing is to make sure that the CPU is always busy and improved response time for every job
        - CPU will have one job at a time
        - OS will ensure reasonable response time through 
            . swapping, processes swapped in and out of memory to disk
            . virtual memory, a technique that allows the execution of program that is larger than physical memory size
            . file system
            . disk management
            . protecting resources from inappropriate use
            . job synchronization and communication
            . handling deadlocks
    = what is multiprocessing
        - multi processor(CPU) systems
        - any CPU will have one job at a time
    = objective of multiprogramming and time-sharing
        - multiprogramming: maximum CPU utilization
        - time-sharing : maximum CPU utilization + maximize users interaction with programs
# operating-system operations
    = operating systems are interrupt driven
    = events are always signaled by the occurrence of an interrupt or a trap
    = trap is a software generated interrupt caused by an error(divide by zero) or by a specific request from a user program
    = dual-mode and multimode operation
        - a system should distinguish between the execution of operating system code and user defined code
        - most popular approach is to provide hardware support
        - 2 separate modes of operations are 
            . kernel mode (also called supervisor mode, system mode, privileged mode), 0 the mode bit
            . user mode, 1 the mode bit
        - a bit(mode bit) is added to the hardware to indicate the mode kernel(0) or user(1)
        - with the mode bit we can distinguish between a task that is executed on behalf of the operating system or the user 
        - when a computer system is executing on behalf of the user application the system is in user mode
        - user application requests a service from the operating system via a system call the system will transition to kernel mode
        <img src="images/1.12.transitionfromkernelmodetousermode.jpg" alt="" width="35%" height="25%">
        - privileged machine instructions, those machine instructions that may cause harm to OS
        - dual-mode, only 2 modes(user mode and kernel mode). privileged machine instructions can execute only on kernel mode else treat it as illegal if on user mode. eg, controlling I/O devices
        - multi-mode, more than 2 modes(CPU uses more than one bit). Intel 64 family of CPUs supports four privilege levels
    = timer
        - timer set to interrupt the CPU after a specified period of time 
        - period may be fixed(1/60 seconds) or variable value, simple technique is to initialize a counter with the amount of time that a program is allowed to run
        - implementation by a fixed clock and a counter
        - every time clock ticks the counter is decremented
        - when the counter reaches 0 the interrupt occurs
        - timer prevent a user program from running too long
# process management
    = a program in execution is process or run time picture of a program is the process
    = a program that is running is a process 
    = a process needs certain resources (CPU time, memory, files and I/O devices) to accomplish its task
    = two type of processes are 
        - single threaded process has one program counter(PC)
        - multi thread processes has multiple PCs
    = process is a unit of work in a system, like centimeter is the unit of height
    = OS is responsible for
        - scheduling processes and threads on the CPU
        - creating and deleting system and user processes
        - suspending and resuming processes
        - providing mechanisms for process synchronization and communication
# memory management
    = main memory is a large array of bytes, each byte having its own address
    = main memory is generally the only large storage device that the CPU is able to address and access directly
    = CPU reads instructions from main memory during the instruction-fetch cycle
    = CPU reads and writes data from main memory during the data-fetch cycle
    = OS is responsible for
        - keep track of which part of memory is currently being used and who is using them
        - deciding which (part of) processes and data to move in and out of memory
        - allocation and deallocating memory space as needed
# storage management
    = provides a uniform, logical view of information storage
    = defines a logical storage unit called 'file'
    = maps these files onto physical media 
    = file-system management
        - OS is responsible for
            . creating and deleting files
            . creating and deleting directories to organize files
            . supporting the manipulation of files and directories
            . mapping files onto secondary storage
            . backing up files on stable storage media
    = mass-storage management
        - because main memory is small and volatile the computer system must provide secondary storage
        - OS is responsible for
            . free space management
            . storage allocation
            . disk scheduling
    = caching
        - information is normally kept in some storage system(such as main memory) 
        - the required information will take directly from cache, if its not available we use the information from the source putting a copy in to the cache
        - performance of various levels of storage
        <img src="images/1.13.performanceofvariouslevelsofstorage.jpg" alt="" width="30%" height="15%">
        . migration of integer A from disk to register
        <img src="images/1.14.migrationofintegerAfromdisktoregister.jpg" alt="" width="20%" height="14%">
    = I/O systems
        - managing and hiding the peculiarities of specific hardware devices through I/O subsystem
        - I/O subsystem consists of
            . memory management components including buffering, caching and spooling
            . general device-driver interface
            . device-driver for specific hardware devices
            <img src="images/1.15.kernel-io-subsystem.jpg" alt="" width="25%" height="15%">
# protection and security
    = a computer with multiple users and executing multiple process, then access to data needs to be regulated
    = protection is any mechanism for controlling the access of processes or users of the resources defined by a computer system
# computing environments
    = traditional computing
    = mobile computing
    = distributed systems
    = client server computing
    = peer-to-peer computing
    = virtualization
    = cloud computing
    = real time embedded systems
<hr>
<b id="id_operatingsystemstructures_detail">operating-system structures</b>
# operating-system service
<img src="images/2.1.operating-systems-services.jpg" alt="" width="30%" height="10%">
    = services provides functions that are helpful for the users
        - user interface   
            . command line interface(CLI)
                * command interpreter
                * uses text commands
                * method for entering the commands (keyboard)
            . batch interface
                * commands and directives to control these commands are entered into files
                * execute these files
            . graphical user interface(GUI)
                * a window for selecting from menu
                * keyboard for entering the text
        - program execution
            . load the program to memory and execute 
        - I/O operation
            . user can't control I/O directly due to efficiency and protection
            . running program requires I/O
        - file-system manipulation
            . running program needs to read and write files and directories
        - communication
            . one process needs to pass information with another process on same machine or another machine that may connected together by a computer network
            . communication may be implemented through shared memory or message passing
        - error detection
            . error may occur in the CPU, memory hardware, I/O device, user program
            . OS needs to take appropriate action like halting error-causing process or halt the system
    = services provides functions that are helpful for the efficient operation of the system
        - resource allocation
            . resource should be allocated to every user and every process running in OS
        - accounting
            . keep track of which users use how much and what resources
        - protection and security
            . owners of the information which is stored in a multiuser or a computer networked environment needs to control the use of that information
# user and operating-system interface 
    = command interpreter or CLI
        - through CLI users can interface with OS
        - some OS include command interpreter in kernel
        - windows and UNIX treat the command interpreter as a special program
        - interpreter known as shells
        - UNIX have bourne shell, c shell, bourne-again shell, korn shell
        - main function of the shell is to execute the user specified commands
        - these commands are implements in 2 different ways
            . bundle the command code along with command interpreter
            . command code as system programs(UNIX implemented). in unix 'rm file.txt', search and execute the file named 'rm' and 'file.txt' as the parameter
        - historically Mac OS has not provided a command line interface, however in the later releases both CLI and GUI are bundled
    = graphical user interface(GUI)
        - through GUI users can interface with OS
        - its have a user friendly interface mouse based window-and-menu system characterized by a desktop metaphor
        - GUI first appeared in Xerox Alto computer in 1973
        - popularized through apple macintosh from 1980
        - microsoft windows 1.0 was based on the addition of a GUI interface to the MS-DOS OS
        - unix includes common desktop environment (CDE), X-windows systems, KDE, GNOME
        - apple includes Aqua with Max OS x
    = mobile and tablet devices implemented 'gestures' on touch screen for the interaction with OS like pressing, swiping fingers across the screen

# system calls
    = system call provide an interface to the services made available by operating system
    = these calls generally available as routines in c, c++ 
    = but certain low level tasks are written in assembly language instructions
    = list of system calls for reading a file contents and write to another file
    <img src="images/2.2.listofsystemcalls.jpg" alt="" width="15%" height="15%">
    = even a simple program will make heavy use of OS and systems will execute 1000nds of system calls per second
    = application programmer design programs according to the application programming interface(API)
    = difference between API and system call
        - API helps to exchange data between various systems, devices and applications
        - system call allows a program to access services from the kernel of the operation system
        <img src="images/2.3.APIvsSystemcall.jpg" alt="" width="35%" height="40%">
        - behind the scene the functions that make up an API typically invoke the actual system calls on behalf of the application programmer
        - most common APIs are
            . Windows API
            . POSIX API for POSIX based systems(UNIX, Linux, and Mac OS X)
            . Java API runs on JVMs
        - programmer access the API via library of code provided by OS. incase of Unix and Linux the library called 'libc'
        - each OS has its own name for each system call
        - why application programmer prefer programming according to API
            . program portability(programs using API suppose to be break free even on different architecture machines)
    = exercise, get the list of system calls in windows, linux, mac and unix ?
# types of system calls
    = process control
    = file manipulation
    = device manipulation
    = information maintenance
    = communications
    = protection
# system programs
    = provide a convenient environment for program development and execution 
# operating-system debugging
    = debugging is the activity of finding and fixing errors and bugs(performance problems) both in hardware and software
    = when process fails, 
        - error information will be written to a file, log file
        - memory of the process will be written to a file, core dump. memory is referred to as 'core' in early days
    = a failure in kernel is called crash. when crash occurs, 
        - error information is saved to a file, log file
        - memory state will be written to a file, crash dump
    = dumps can be probed by a debugger
    = performance tuning, to improve performance by removing processing bottlenecks
    = to identify bottlenecks 
        - we need to monitor system performance
        - we need to have a methods of computing and displaying measures of system behavior
    = solaris 10 new generation kernel enabled performance analysis tool, DTrace, dynamic tracing facility
    = DTrace will dynamically adds probes to a running user processes and in the kernel
# operating-system generation
    = in order to design to run at variety of sites with a variety of peripheral configurations 'sysgen' program is executed
    = this 'sysgen' program reads from a given file
    = given file will have all the component details
        - what CPU is to be used
        - how will the boot disk tbe formatted
        - how many memory is available
        - what devices are available
        - what OS options are desired(how many buffers and of what size, CPU-scheduling algorithm, max number of processes to be supported)
    =  with these data, OS is then completely recompiled and can implement an OS specifically for a site
# system boot
    = the procedure of loading the kernel into memory is known as booting
    = a small piece of code known as bootstrap program 
        - runs diagnostics to determine the state of the machine
        - initialize all aspects of the system(from CPU registers to device controllers)
        - locates the kernel
        - loads the kernel to memory
        - starts execution
    = the bootstrap program will be on ROM and its called 'firmware'
    = firmware, because its characteristics fall somewhere between those of hardware and those of software
    = GRUB is an example of an open-source bootstrap program for linux systems
    = a disk that has a boot partition is called boot disk or system disk
<!-- </details> -->
<hr>
<b id="id_processes_detail">processes management</b>
<b id="id_processes_detail">processes</b>
# process concept
    = process is program in execution or runtime picture of a program
    = process can be called a job or a task
    = process contains following sections
    <img src="images/3.1.processinmemory.jpg" alt="" width="10%" height="5%">
        - text section : code and value of processor's registers
        - data section : global variables
        - heap section : area for dynamic allocation
        - stack section : temporary data area(function parameter, return address and local variables)
    = a program become process when the program loaded in to memory
    = 2 common techniques for loading executable files to memory
        - double clicking an icon representing the executable file
        - entering the name of the executable file on CLI
    = process itself can be a running environment for code, JVM process, java code is executed inside JVM
    = as a process executes, it changes state, in one of the following
    <img src="images/3.2.processstate.jpg" alt="" width="25%" height="5%">
        - new, the process is being created
        - ready, waiting to get the processor
        - running, instruction is being executed
        - terminated, process finished execution
        - waiting, for some event to occur(IO completion or reception of a signal) 
    = each process is represented in the OS by a process control block(PCB, task control block)
    = PCB is a data structure used by OS to store all the information about a process
    <img src="images/3.3.processcontrolblock.jpg" alt="" width="10%" height="5%">
        - process state : new, running, terminated, etc
        - process number or id
        - program counter : address of the next instruction to be executed for this process
        - registers : accumulator, index registers, stack pointer, general purpose register and any condition code information. PC and other state information needs to be registered when an interrupt occurs.
        - CPU scheduling information : process priority, pointers to scheduling queues and other scheduling parameter
        - memory management information : bas and limit registers and page tables or segment tables
        - accounting information
        - IO status information : list of IO devices allocated to the process, list of open files 
# process scheduling
    = process scheduler select an available process for execution on the CPU
    = processes entering to the system will put into a job queue
    = 2 type of queues are present 
       - processes residing in main memory(ready and waiting) are kept on a list called 'ready queue'
       - processes waiting for a particular IO device is called a 'device queue'. each device has its own queue.
    = data structure for ready queue is generally on linked list
    = head part contains pointers to the first PCB and tail part contains pointers to the final PCB on ready queue header
    = each PCB includes a pointer field that points to the next PCB in the ready queue
    <img src="images/3.4.variousqueues.jpg" alt="" width="20%" height="5%">
    = process is getting allocated to CPU from ready queue
    <img src="images/3.5.queuingdiagramofprocessscheduling.jpg" alt="" width="23%" height="5%">
    = process migrate among various scheduling queues throughout its life time. OS must select processes from these queues
    = the selection process is carried out by the appropriate 'process schedulers'
    = 3 type of schedulers are 
        - long term scheduler, select the process from job pool(typically from mass storage) and loads to memory
        - medium term scheduler, to remove a process from main memory to secondary memory(swap out) and later reintroduced to main memory from secondary memory(swap in)
        - short term scheduler, select for CPU execution
        <img src="images/3.6.typesofschedulers.jpg" alt="" width="23%" height="5%">
    = 'frequency of execution' is the differentiation factor between schedulers
        - often short term scheduler will execute once every 100 milliseconds
        - ofter long term scheduler will execute once in minutes and so
    = 2 types of processes
        - IO bound process, more time spent on IO than on computation
        - CPU bound process, more time doing computation and generate infrequent IO request
    = lond term scheduler needs to select a good process mix but short term scheduler will have little to do
    = UNIX and Windows often no long term scheduler instead put every new process in memory for the short term scheduler
    = an alternative is medium term scheduler, will remove process from memory and later this can be reintroduced for execution, this scheme is called swapping
    = context switch
    = altering user task and kernel task is frequent in general purpose system
    = when an interrupt occurs, the current 'context' of the process running on the CPU is saved
    = context is represented in PCB of the process
    = 'context switching', performing the 'state save' and 'state restore'
    = context switching is an overhead on systems
    = Sun UltraSparc provides multiple sets of registers, so context switching is just pointing to a different register set
# operations on processes
    = process creation 
        - during the course of execution, a process may create  several new processes 
        - every process will be identified with a unique number called process identifier(pid)
        - process creator process is called parent process 
        - new process is called child process
        - when a process creates a new process, 2 possibilities exists
            . the parent continues to execute concurrently with the child
            . the parent wait for the child to get terminated
        - address space is a range of valid addresses in memory that are available for a process
        <img src="images/3.7.addressspace.jpg" alt="" width="23%" height="5%">
        - 2 address space possibilities for a new process
            . child is a duplicate of parent process, so it has the same program and data as the parent
            . child process has a new program got loaded into it
        - UNIX way of new process creation
            . process are created by fork() in UNIX in that a parent creates creates a new child process 
            . contents of new process's address space will be a copy of parent process's address space
            . since both address spaces are identical, its difficult to identify parent and child process
            . return value of fork() is useful for finding the parent and child process (pid = fork())
                * pid = 0 in child process
                * pid = nonzero in parent process 
                <a href="https://www.csl.mtu.edu/cs4411.ck/www/NOTES/process/fork/fork-02.c" target="_blank" rel="noopener noreferrer">explaining forking with example</a>
            . both process will continue to execute at the very next instruction after the fork() code line
            . exec() system call loads a binary file into memory and starts execution 
            . after exit the child process the parent process will re-start again
            <img src="images/3.8.processcreationusingforksystemcall.jpg" alt="" width="30%" height="10%">
        - Windows way of new process creation  
            . process are created in the Windows API using CreateProcess() function
            . the child process will be loading a specified program into its address space
    = process termination 
        - child process finishes (exit() system call) its execution and return the status value to the parent process 
        - exit() will deallocate or closes all the open files, IO buffers and virtual and physical memory ie PCB removed by the OS
        - parent process will be waiting (wait() system call)
        - functions of wait() system call in 2 different situations
            . if at least one child process running and the wait() call made, then the caller will wait until that child exit() its process
            . if there is no child process is running and the wait call made, then it will have no effect
        - child process will pass its identity to parent process  
        - only parent can initiate a termination system call (TerminateProcess()) to avoid inappropriate use 
        - a parent can initiate the termination process due to
            . child exceed the resource usage 
            . task assigned to child is no longer required
            . OS will not allow the child to continue when its parent exited, this phenomenon is referred to as 'cascading termination'
        - normal termination circumstances are
            . directly by exit() system call
            . indirectly by return from main() function call
        - process table, is a data structure maintained by the OS to facilitate context switching, scheduling and other activities
        - process table is an array of PCBs 
        - each entry in the table is called 'context block'
        <img src="images/3.9.processtable.jpg" alt="" width="20%" height="10%">
        - even after the exit() call the entry in the process table will remain until parent calls wait(), since this table contains process exit status 
        - a process that is terminated and parent not yet called the wait() is known as 'zombie'
        - process identifier of the zombie process ans its entry in the process table are released
        - if the parent not invoked the wait() call but the child terminated these processes are called 'orphans'
        - Linux and UNIX address this scenario by assigning the init process as the new parent to orphan processes
        - init will invoke wait() periodically to collect exit status of the orphaned process and releasing the orphan's process identifier and process table-entry
# interprocess communication
    = 2 types of process are
        - independent process, no data sharing with other process
        - cooperating process, data sharing with other process
    = cooperating process require an interprocess communication(IPC) mechanism to exchange data and information 
    = 2 models of IPC   
        - message passing, sharing takes place by means of message exchange 
        - shared memory, region of memory is shared between cooperating process
        <img src="images/3.10.interprocesscommunication.jpg" alt="" width="20%" height="5%">
    = both models are common in OS
    = message passing is easy for 
        - small amount of data sharing
        - distributed systems
    = message passing require more system calls compared to shared memory as this requires system call only to establish shared memory region 
    = on single core systems, preferred model is shared memory 
    = on multi core systems, preferred model is message passing. shared memory model suffers from cache coherency issues 
    = cache coherency, ensures that changes to the values ​​of shared operands are propagated across the system in a timely manner
    = shared memory model
        - communicating process(process A in picture) will establish a region to share with other processes 
        - usually, a shared memory region resides in the address space of the process that creates the shared memory segment
        - other process wish to communicate using this shared memory segment must attach it to their address space
        - OS will prevent one process to access other processes memory 
        - shared memory requires that 2 or more process agree to remove this restriction 
        - common paradigm for illustrate the concept of cooperating processes is producer-consumer problem
            . compiler produces assembly code and consumed by assembler
            . assembler produces object modules and consumed by loader
            . server produces web pages and browser reads those web pages
        - producer will create a buffer in its own address spaces
        - producer and consumer will agree to read and write to this buffer
        - producer and consumer must be synchronized
        - 2 types of buffers 
            . unbounded buffer, no practical limit on the size of the buffer
                * producer can always produce items
                * consumer must wait if the buffer is empty   
            . bounded buffer, fixed buffer size
                * producer must wait if the buffer is full
                * consumer must wait if the buffer is empty     
    = message passing model
        - process should send a message and other process should receive it
        - a communication link needs to establish for the above communication 
        - few methods for implementing this communication link 
            . direct and indirect communication 
            . synchronous or asynchronous communication 
            . automatic or explicit buffering
        - direct communication
            . create an identity(name) for the sender and receiver
            . every pair will have exactly one link 
            . one variant, sender and receiver process uses others name to communicate, symmetry
            . another variant, only sender uses receivers name for communication, asymmetry
            . this method can be termed as hard coding technique 
        - indirect communication
            . messages are sent to and receive from mailboxes or ports 
            . each mailbox has a unique identification
            . every pair will have a shared mailbox 
            . mailbox may be owned by either OS or by a process(part of address space)
            . if the owner of the mail box terminates, then the mail box also needs to be terminated 
        - synchronous or asynchronous communication 
            . send and receive of messages are 2 primitives for this message passing model
            . message passing may be either blocking(synchronous) or non blocking(asynchronous)
                * blocking send, send the next message only after the completion of the current message is delivered
                * non blocking send, send the message irrespective of delivery status
                * blocking receive, wait until the message is available 
                * non blocking receive, keep the receiving loop always on
        - automatic or explicit buffering
            . messages are exchanged through a queue 
                * zero capacity(no buffering), no waiting in queue 
                * bounded capacity(automatic buffering), queue has finite length. sender must stop producing if its full
                * unbounded capacity(automatic buffering), queue has in-finite length. sender can keep producing
# communication in client–server systems
    = 3 other process communication techniques apart from message passing and shared memory
        - sockets
        - remote procedure calls(RPCs)
        - pipes 
    = socket, is defined as an endpoint for communication 
        - socket communication is considered a very low level form of communication between distributed processes 
        - a pair of process communicating over a network employs a pair of sockets, one for each process 
        - each socket is identified with an IP address concatenated with a port number 
        - a host machine is referring to itself with an IP address of 127.0.0.1 called loopback
        - socket uses a client server architecture 
        - server waits for incoming client requests on a port 
        - once a request is received, the server accepts a connection from the client socket to complete the connection 
        - server implement specific services to listen to on well know ports
        - port, is a logical construct that identifies a specific process
        - well known ports are, all the ports below 1024(telnet on port 23, ftp on 21, HTTP on 80)
        - when a client process initiates a request for a connection, host machine assigns a port
        - host machine will assign an arbitrary number greater than 1024 as the port
        - socket allow only an unstructured stream of bytes to be exchanged and server and client application to impose a structure on the data 
    = remote procedure calls
        - RPC communication is considered a high level form of communication between distributed processes 
        - this method uses a message based communication scheme to provide remote service 
        - message exchanged in RPC communication are well structured and are no longer just packets of data 
        - message to the server(service) is  
            . addressed to an RPC daemon listening to a port on the remote system 
            . an identifier specifying the function to execute and the parameters to pass to that function 
        - message response to the client is 
        . output will be send to the client in a separate message 
    <img src="images/3.11.executionofRPCcall.jpg" alt="" width="25%" height="5%"> 
    = pipes 
        - pipe acts as a conduit/channel allowing 2 processes to communicate 
        - one of the simplest way for IPC 
        - implementation points to consider 
            . pipe is unidirectional or bi-directional 
            . if bi-directional
                * half-duplex(data can travel only one way at a time)
                * full-duplex(data can travel both ways at same time)
            . should a parent-child relationship exist before communication 
            . communication over a network is possible 
        - ordinary pipes
            . communication in a producer-consumer fashion in unidirectional model with a parent and child relationship
            . producer writes to one end of the pipe and consumer reads from the other end 
            . 2 pipes required for implementing bi-directional communication
            . UNIX treats a pipe as a special type of file 
            . parent process creates pipe and fork() ing a child, so the child will have access to pipe created by parent   
            . ordinary pipe can only be used for processes with a machine 
            . once the process has finished communication and terminated the ordinary pipe ceases to exist 
            . 'ls | more' in UNIX and 'dir | more' in Windows 
        - named pipes 
            . communication will be in bi-directional model with out any parent-child relationship 
            . once a named pipe is established, several processes can use it for communication 
            . in a typical scenario, a named pipe can have several writers 
            . even after the communicating processes have finished the named pipe continue to exist 
<!-- </details> -->
<hr>
<b id="id_threads_detail">threads</b>
# overview
    = a thread of execution is the smallest sequence of programmed instructions that can be managed independently by a scheduler 
    = 1 or more threads are executing on a processes address space
    = a sequence of control within a process 
    = it is not 'thread' but 'threaded process', process with single thread or multiple threads 
    = thread is a basic unit of CPU utilization
    = a thread includes its own
        - thread ID
        - program counter
        - register set
        - stack
    = a thread shares the following with other threads in the same process
        - code section
        - data section 
        - other OS resources (such as open files and signals)
    <img src="images/4.1.singleandmultithreadedprocess.jpg" alt="" width="25%" height="9%">
    = traditional process is also known as heavyweight process and it has only a single thread of control 
    = a web server process will have multiple threads
        - one thread display text or images 
        - another thread retrieves data from network
    = 2 ways of handling client requests on a web server
        - web server runs as a single threaded process to listen for requests. server creates a separate process for each request. This process-creation method is time-consuming and resource-intensive 
        - web server run as multithreaded process and create separate threads to service new requests
        <img src="images/4.2.multithreadedserverarchitecture.jpg" alt="" width="30%" height="9%">
    = several threads operate in most of the operating system kernel, each thread performs a specific task such as managing devices, managing memory or interrupt handling  
    = benefits of threads 
        - responsiveness, when a user clicks a button 
            . on a single threaded application, the application will be nonresponsive until the user request is completed
            . on a multithreaded application, the application will be responsive as the user request is working as a new thread and application another 
        - resource sharing, process share the resources to multiple threads on the same address space 
        - economy, allocating memory and resources for process creation is costly. in solaris process creation is about thirty times slower than thread creation  
        - scalability, single threaded process can run only on one processor but multithread process can run on multiple processors on multicore systems  
# multicore programming 
    = multicore is a microprocessor on a single integrated circuit with 2 or more separate processing unit called cores
    <img src="images/4.3.multicoremultiprocessor.jpg" alt="" width="20%" height="9%">
    = sequential, concurrency and parallelism 
        - sequential, a task is a unit. after completing one task, start another task 
        - concurrency, a slice of a task is a unit. CPU scheduler switches rapidly between processes for execution so that every process will have progress
        - parallel, a slice of a task is a unit. if it can execute more than one task simultaneously
        <img src="images/4.4.concurrency.jpg" alt="" width="30%" height="9%">
    = opportunities for the programmers in multicore systems  
        - identifying of tasks that are independent of one another and can run in parallel on individual cores
        - balancing on the identified tasks which will do equal work of equal value so that the cores are occupied equally
        - data splitting like the application is divided to tasks even the data needed to be divided as per the tasks  
        - data dependency, execution of tasks needs to be synchronized if the data is depended on other tasks 
        - testing and debugging, all the execution paths needs to be tested 
    = types of parallelism
        - data parallelism, same task and a subset of data set are distributed among multiple cores. distribution of data among multiple cores 
        - task parallelism, different tasks and a subset of data set/different data set are distributed among multiple cores. distribution of tasks among multiple cores
    = most applications use hybrid approach on selecting types of parallelism 
    = support for threads can be provided in
        - user level (user thread), managed by thread library in user space and with out kernel support 
        - kernel level (kernel thread), managed directly by operating system 
# multithreading models 
    = many to one 
        - maps many user threads to a kernel thread 
        - disadvantage is one long running IO request to kernel will block entire process 
        - only one thread can access the kernel at a time so multicore advantages can't be used 
    = one to one 
        - maps each user thread to kernel thread 
        - multicore advantages can be used
        - windows and linux systems have implemented this model 
    = many to many 
        - many user level threads to a smaller or equal number of kernel threads
    = two-level
        - hybrid of many to many and one to one
        - many user level threads to a smaller or equal number of kernel threads
        - but also allows one to one
        <img src="images/4.5.multithreadingmodels.jpg" alt="" width="30%" height="9%">
# thread libraries
    = thread libraries provides the programmer with an API for creating and managing threads 
    = 2 ways for implementing a thread libraries
        - provide a library entirely in user space without kernel support. code and data structures are in kernel space. invoking a function in the library results a local call not a system call  
        - provide a kernel level library supported directly by OS. code and data structure are in user space. invoking a function in the API typically results a system call 
    = 3 main thread libraries 
        - Pthread (POSIX thread), provided as either a user level or a kernel level library. this is POSIX specification not implementation 
        - windows, provided as a kernel level library
        - Java, library calls to JVM but internally call to host OS
    = 2 general strategies for creating multiple threads 
        - synchronous threading
            . parent creates one or more children and waits for all the children to finish execution then resume operation
            . parent is depended by children are independent 
            . fork-join strategy
            . eg, parent thread may combine the results calculated by its various children 
        - asynchronous threading 
            . parent creates one or more children and both execute concurrently
            . parent and children threads are independent 
            . ideally a very little data sharing 
        <img src="images/4.6.syncronousandasynchronousthreading.jpg" alt="" width="20%" height="9%">
# implicit threading
    = let compiler creates and manages the threads instead of application developers
    = 3 alternative approaches are
        - thread pools 
            . create a number of threads at process startup, place them into a pool and threads will wait for work
            . when a server receives a request, it awakens a thread from the pool 
            . once a thread completes its services, it returns to the pool 
            . if no thread is available in the pool, the server waits until one becomes free  
        - openMP
            . it is a set of compiler directives as well as an API for programmers written in C,C++ and fortran 
            . provide support in shared memory environment
            . openMP identifies parallel regions as blocks of code that may run in parallel 
            . application programmer insert compiler directives into their code at parallel regions 
            . these directives will instruct openMP run-time library to execute those regions in parallel  
            . openMP is available on several open source and commercial compilers for Linux, windows and Mac OS X systems 
        - grand central dispatch
            . a technology for Apple's MAC OS X and iOS OS 
            . it is a combination of C language, an API and a run-time library 
            . GCD manages the details of threads like openMP 
            . GCD directives mark a block as a self contained unit of work 
        - Intel's threading building blocks(TBB) 
        - java.util.concurrent  
# threading issues
    = the fork() and exec() system calls
        - fork() call will duplicate parent process 
        - some UNIX systems chosen to have 2 versions of fork()
            . new forked process will have all the threads duplicated
            . new forked process will have only one thread duplicated, ie the invoking thread 
        - exec() call will invoke a new program
        - if a thread calls exec()
            . replace entire process including all threads 
        - consideration issues is that which fork() version needs to be selected
        - if fork() is followed by an exec() then duplicating all threads are unnecessary
        - if fork() is not followed by an exec() then duplicating all the threads are necessary  
    = signal handling 
        - signal : event generated by the software that are running in the CPU, signal is used in UNIX to notify the process that an event has occurred
        - steps of signal 
            . a signal is generated by the occurrence of a particular event 
                * illegal memory access or division by zero
                * terminating process by pressing ctrl+c or timer expire
            . signal is delivered to a process synchronously or asynchronously
                * synchronous, generation of an event and consumption of the signal by same process
                * asynchronously, generated by an external event and consumption of the signal by another process
            . handling the signal by the process, handled by one or two possible handlers
                * default signal handler
                * user-defined signal handler
        - a default signal handler is associated with every signal and it might take a default action like
            . terminate the process
            . ignore the signal
            . dump core 
            . stop the process 
            . continue the stopped process 
        - user defined signal handler, will overrides default action
        - signal can be delivered in a multithreaded environment
            . to the thread to which the signal applies
            . to every thread in the process 
            . to certain threads in the process
            . assign a specific thread to receive all the signals for the process  
    = thread cancellation 
        - terminating a thread before it has completed 
        - the thread to be cancelled is called target thread 
        - cancellation can be of 2 scenarios 
            . asynchronous cancellation, one thread immediately terminates target thread 
            . deferred cancellation, the target thread periodically checks whether it should terminate 
    = thread local storage 
        - thread needs its own copy of data which is called TLS 
        - thread unique id is stored in TLS 
    = scheduler activations 
        - for implementing many to many and two level models a data structure is placed between user threads and kernel threads and this data structure is called light weight process(LWP)
        <img src="images/4.7.lightweightprocess.jpg" alt="" width="5%" height="3%">
        - LWP can be considered as a virtual processor on which application can schedule a usr thread to run 
        - an application may require any number of LWPs to run efficiently
        - CPU bound application running on a single processor(one thread) require one LWP but IO intensive application may require multiple LWPs to execute 
        - one scheme for communication between user thread library and the kernel is known as scheduler activation 
        - kernel must inform application about certain events and this procedure is known as 'upcall'. upcalls are handled by the thread library with an upcall handler which is running on the LWP 
        - one event that triggers an upcall occurs when an application thread is about to block 
        - kernel then makes an upcall to the application informing it that a thread is about to block and identifying the specific thread
        - kernel then allocates a new LWP to the application 
# extra
    = difference between signals and interrupts
        - signal : event generated by the software that are running in the CPU
        - interrupt : event generated by the external components other than CPU 
    = POSIX : Portable Operating System Interface is a family of standards specified by the IEEE Computer Society for maintaining compatibility between OSs
<hr>
<b id="id_processsynchronization_detail">process synchronization(avoid data inconsistency)</b>
# overview
    = concurrent access to shared data may result in data inconsistency
    = synchronization refers to one of the 2 distinct but related concepts   
        - process synchronization
        - data synchronization
    = process synchronization refers to the idea that multiple processes are to join up or handshake at a certain point in order to reach an agreement or commit to a certain sequence of action 
    = data synchronization refers to the idea of keeping multiple copies of a dataset in coherence with one another or to maintain data integrity
    = race condition, several process access and manipulate same data concurrently and the outcome will depends on a particular order
        - to guard against the race condition we need to ensure that only one process at a time can manipulate a variable or a block 
        - to avoid race condition OS needs to synchronize the processes 
# critical-section
    = critical section, a segment of code in which process may change common variables, updating a table or writing a file etc
    = critical section problem, is to design a protocol that 2 processes will not be allowed to execute the critical section simultaneously
    = each process must request permission to enter its critical section. section of code implementing this request is 'entry section'
    = followed by 'critical section', 'exit section' and remaining code is 'remaining section'
    <img src="images/5.1.criticalsection.jpg" alt="" width="8%" height="9%">
    = every critical-section solution must satisfy following 3 requirements
        - mutual exclusion(exclusively only one process), at any point of time only one process should be present inside critical section   
        - progress(competition is only between the processes those wanted to enter the critical section, other process will not be participating) do not postpone the selection indefinitely
        - bounded waiting(no live lock, limited waiting to enter critical-section), the waiting time of the process to enter the critical section should be limited after the request 
    = 2 approaches used to handle critical section in OS are 
        - preemptive kernel : allows a process to be removed and replaced while running in kernel mode. this approach is more preferable in real-time programming 
        - non preemptive kernel : does not allow the process to be preempted and it will run until the process exit, block or voluntarily yields control of the CPU 
# peterson’s solution
    = software-based solution to the critical-section problem 
    = there is no guarantee that peterson’s solution will work on modern computer architecture but it provides a good algorithmic description of solving the critical section
    = this solution works only for 2 process which is contesting for critical section. these 2 processes will share 2 variables(flag[2] and turn)
    = how it works
        - pi and pj are the 2 processes which is currently executing in their entry sections 
        - let's say the order of execution is pi then pj 
        - pi executes and sets the shared variable 'turn' as j
        - pj executes and sets the shared variable 'turn' as i
        - on the next time quanta of pi, 'while' statement will be false and enter its critical section meanwhile pj waits on its 'while' loop 
        - after completing the critical section on pi will set 'flag[i] = false'
        - on the next time quanta of pj, 'while' statement will be false and enter its critical section meanwhile pi will be on its remainder section 
    = this solution satisfies all 3 requirements 
    <img src="images/5.2.petersonssollutionimplementaion.jpg" alt="" width="30%" height="9%">
# hardware based solutions 
    = privileged instructions inbuilt in CPU
    = 2 hardware instructions are 
        - test and set, modify the content of a word
        - compare and swap, swap content of 2 words
    = all the above operations are atomic operations
    = these solution can work for many processes 
    = how 'test and set' solution works
        - a shared variable called 'lock' will be used and data type will be boolean and initialized to false 
        - first check or test the value of lock variable
            . if lock = true(1), process will wait 
            . if lock = false(0), process will set lock = true(1) and enter the critical section  
        - after setting lock = 1 then enter the critical section
        - all other process will be waiting the lock
    = does it meet the requirements? 
        - satisfies mutual exclusion requirement
        - does not satisfies bounded waiting requirement
    <img src="images/5.3.testandsetlockimplementaion.jpg" alt="" width="40%" height="9%">
    = how 'compare and swap' solution works
    <img src="images/5.4.compareandswapimplementaion.jpg" alt="" width="40%" height="9%">
    = does it meet the requirements? 
        - satisfies mutual exclusion requirement
        - does not satisfies bounded waiting requirement
# mutex locks
    = another software based solution 
    = mutex - mutual exclusion 
    = acquire() function acquires the lock and release() function releases the lock
    = main disadvantage of the above implementation is 'busy waiting' or 'spin lock', since the CPU will be busy of 'spin lock'
    = advantage of spin lock is that no intermediate context switch is required which is costly 
    = this method is implemented on multiprocessor system where one thread can spin on one processor while another thread performs critical section on another processor
    = mutex is used on Pthreads 
    <img src="images/5.5.muteximplementaion.jpg" alt="" width="25%" height="9%">
# semaphores
    = another software based solution 
    = provide more sophisticated way than mutex 
    = this solution make use of one integer variable called semaphore S 
    = 2 types of semaphores
        - counting semaphores(vast range of numbers)
        - binary semaphores (0 or 1), can be used instead of mutual exclusion 
    = counting semaphores, initialized to the number of resources 
    = each process wants to use the resource first perform wait() then critical section and signal() 
    = wait() is termed P for test 
    = signal() is termed V to increment 
    <img src="images/5.6.semaphore.jpg" alt="" width="25%" height="9%"> 
# problems of synchronization
    = 3 common synchronization problems
        - bounded buffer problem (producer-consumer problem)
        - readers writers problem 
        - dining philosophers problem 
    = solution for bounded buffer problem using semaphores
        - a buffer of fixed number of slots and each one is capable of storing one unit of data 
        - bounded buffer ie fixed buffer size
        - bounded buffer problem statement
            . producer must not fill data when the buffer is full
            . consumer should not remove data when the buffer is empty 
            . simultaneous insert and removal should not happen 
        - producer process - fill/insert data to an empty slot
        - consumer process - removes data from a filled slot
        - solution for bounded buffer problem is implemented through 3 semaphores
            . m(mutex), binary semaphore, value to acquire and release locks 
            . empty, counting semaphore, number of empty slots. initial = total number of slots
            . full, counting semaphore, number of filled slots. initial = 0 
        <img src="images/5.7.boundedbufferproblemimplementationusingsemaphore.jpg" alt="" width="55%" height="9%">
    = solution for readers-writers problem using semaphores
        - readers-writers problem relates to shared object such as a file that is shared between multiple processes 
        - readers-writers problem statement and objective
            . a shared object can be accessed by 'n' readers, but only one writer at a time
        - 2 kind of processes 
            . reader processes - only to read
            . writer processes - read and write
        - solution is implemented through 2 semaphores and an integer value
            . readcount, integer variable, how many processes are currently reading the shared object, initial=0
            . mutex, binary semaphore, ensure mutual exclusion when the variable 'readcount' is updated. initial=1 
            . rw_mutex, binary semaphore, ensure mutual exclusion when for writers. initial=1  
            <img src="images/5.8.reader-writerproblem.jpg" alt="" width="25%" height="9%">
    = solution for dining philosophers problem using semaphores
        - problem statement
            . allocating limited resources to multiple processes can cause deadlocks and starvation. so we need to build a solution to address that and allocate resources in a synchronized manner
        - dining philosophers problem is a simple representation of the above problem ie, resource allocation 
        - illustration
            . 5 philosophers who spend their lives thinking and eating 
            . from time to time a philosopher gets hungry and tries to pick up 2 chopsticks that are closest to her
            . philosopher picks only one chopstick at a time, and she requires 2 chopstick to start eat
            . when finish eating, she puts down both chopstick and starts thinking again 
        - philosopher chooses 2 of her adjacent chopsticks. implement this solution via the semaphore 
            . term each chopstick as a semaphore(chopsticks[5]), binary semaphore, all chopsticks initial=1 
            <img src="images/5.9.diningphilosophersusingsemaphore.jpg" alt="" width="25%" height="9%">
        - above semaphore solution wont avoid deadlocks, so remedies are  
            . allow at most 4 philosophers to be sitting simultaneously at the table 
            . pick up the chopsticks only if both chopsticks are available
            . use asymmetric solution 
                * odd numbered philosopher picks up left then right chopstick 
                * even numbered philosopher picks up her right then left chopstick 
# monitors
    = using semaphore mechanism incorrectly can result in timing errors which is difficult to detect 
    = to deal with these kind of errors researchers have developed a high-level language construct - the 'monitor' type
    = syntax of monitor 
        monitor monitor_name {
            /* shared variable declarations */
            function P1(.....) {
                ......
            }
            function P1(.....) {
                ......
            }
            function Pn(.....) {
                ......
            }
            initialization_code(.....) {
                .....
            }
        }
    = the monitor type is an ADT that contains a set of programmer-defined functions that provide mutual exclusion within the monitor
    = various processes needs to call the functions defined with in the monitor 
    = monitor construct ensures that only one process at a time is active within the monitor 
    = monitor construct is not sufficiently powerful for modeling some synchronization schemes, we may need to use condition variables 
        condition x,y //pseudo code
    = a condition variable in operating system programming is a special kind of variable which is used to communicate between threads when certain conditions becomes true
    = operations that can be invoked on condition variable are wait() and signal()
        x.wait()  // suspending an operation
        x.signal() // resumes a suspended process 
    = example for condition variables written using pthread conditional variables
        pthread_mutex_t m = PTHREAD_MUTEX_INITIALIZER;
        pthread_cond_t x = PTHREAD_COND_INITIALIZER;

        pthread_cond_wait(&x, &m);
        pthread_cond_signal(&x);
    = illustrate a monitor based solution deadlock free solution to the dining philosopher problem
    = this solution imposes the restriction that a philosopher may pick up her chopsticks only if both of them are available
        monitor diningphilosophers {
            enum { THINKING, HUNGRY, EATING} state [5];
            condition self [5];
            
            void pickup (int i) {
                state [i] = HUNGRY;
                test (i) ;
                if (state [i]!= EATING)
                    self [i].wait() ;
            }
            
            void putdown(int i) {
                state [i] = THINKING;
                test ((i + 4) % 5);
                test ((i + 1) % 5);
            }
            
            void test (int i) {
                if ((state [(i + 4) % 5] != EATING) && 
                    (state [i] == HUNGRY) && 
                    (state [(i + 1) % 5]!= EATING)) {
                        state [i] = EATING;
                        self [i].signal() ;
                }
            }
            
            initialization_code () {
                for (int i = 0; i < 5; i++) 
                    state [i] = THINKING; 
            }
        }
    = decide which suspended process to resume within monitor
        - simple solution is FCFS ordering, process waiting longer served first
        - condition wait construct, a priority number is stored with name of the process and process with smallest priority number is resumed next
            x.wait(time)
# synchronization examples
    = <to be filled>
# 3 alternative approaches
    = transactional memory
    = openMP
    = functional programming languages
        - since functional languages ​​don't allow mutable condition, don't have to worry about problems like deadlock or race condition(erlang and scala)
# extra
    = bounded buffer lets multiple producers and multiple consumers share a single buffer. 
      producers write data to the buffer and consumers read data from the buffer. 
      producer may block if the buffer is full and consumers must block if the buffer is empty
    = atomic operation, operation as a single unit, no interruption is allowed during this operation
    = a 'busy wait' or 'spin lock' : waits for a condition to be satisfied in a loop without leaving the processor
    = a language construct is a part of a program formed from one or more lexical tokens according to the rules of a programming language. 
        public class MyClass {
            //code . . . . . .
        }
    = a data structure is a particular way of storing and organizing data in a computer so that it can be used efficiently
    = Abstract Data Types(ADT) are theoretical but when realized are called data structures
    = A data type can be considered abstract when it is defined in terms of operations on it, and its implementation is hidden
    = data type vs user defined data types vs abstract data type
        - data type
            . defined along with the language for storing data like numbers or characters etc and certain types of operations (+,*,-,/)
        - user defined data type
            . defined by user by extending built-in types by using structure, enum, typedef, classes, interfaces etc and certain types of operations
                struct point {
                    int x;
                    int y;
                }
        - abstract data type
            . these are similar to user defined data types (type + functions) but the implementation of functions are hidden from programmer
    = difference of imperative and functional languages
        - imperative, these languages are state based represented by variables which is mutable 
        - functional, these languages don't maintain state, once the variable is set its immutable
<!-- </details>   -->
<hr>
<b id="id_cpuscheduling_detail">cpu scheduling</b>
# basic concepts
    = https://www.quora.com/What-are-the-goals-of-CPU-scheduling ( how computer works)
    = as a result of scheduling CPU
        - improve the utilization of CPU
        - speed of the computer response to its users 
    = CPU scheduling is the process of choosing which process gets the CPU from the ready queue
    = scheduling can be termed as the order of process selection
    = it is kernel level thread not processes that are in fact being scheduled by the operating system to use CPU
    = a process execution consists of cycles of
        - CPU execution(CPU burst)
        - IO wait(IO burst)
    = CPU scheduler
        - whenever the CPU becomes idle, the OS must select one of the processes from the ready queue to be executed 
        - selection of the process is carried out by the short term schedular or CPU schedular 
        <img src="images/6.1.cpuschedular.jpg" alt="" width="25%" height="9%">
    = scheduling methods(preemptive/non-preemptive)
        - circumstances for scheduling decisions 
            . (1) process switches from running to waiting state(waiting for IO request completion or wait() for child process to terminate)
            . (2) process terminates
            . (3) process switches from running to ready state(interrupt occurs)
            . (4) process switches from waiting to ready state(IO completion)
        - in 1 and 2 no choice in terms of scheduling OS must select a new process
        - in 3 and 4 there is a choice of scheduling
        - preemptive, higher priority process will get the preference on CPU allocation(Windows 95,Mac OS X)
        - non preemptive, once the process is allocated it will hold on to the CPU till it gets terminated or moved to waiting state(Windows 3.x)
    = dispatcher
        - dispatcher is a module that gives control of the CPU to the process which is selected by short term schedular
        - functions involved are    
            . switching context 
            . switching to user mode 
            . jumping to proper location in the user program to restart that program 
        - dispatch latency, is the time it takes for the dispatcher to stop one process and start another process
# general scheduling benchmark for comparing CPU scheduling algorithms
    = scheduling criteria/benchmark
        - CPU utilization
            . time that CPU was busy 
            . 40%, lightly loaded systems
            . 90%, heavily loaded systems
        - throughput
            . number of processes completed per time unit 
        - turnaround time 
            . time of submission of a request to the time of completion 
            . sum of waiting to get into memory + waiting in the ready queue + executing on the CPU + doing IO 
        - response time 
            . time from the submission of a request until the first response is produced 
        - waiting time 
            . sum of the time spent waiting in the ready queue 
    = a desirable scheduling algorithm needs to have it is desirable to 
        - maximize, CPU utilization and through put
        - minimize, turnaround time, response time, waiting time
# CPU scheduling algorithms will decide which process in the ready queue allocate to the CPU 
    = first come first serve scheduling (non preemptive)
        - process that request the CPU first gets the CPU first 
        - implementation is easily managed with a FIFO queue 
        - the new PCB will be added to the tail of the ready queue
        - the CPU will be allocated to the PCB on the head, running process will be removed from the ready queue 
        - on the negative side, the average wait time can vary significantly if the CPU burst times of the processes vary greatly  = shortest job first scheduling 
        - convoy effect, all the other processes wait for one big process to get off the CPU 
        - FCFS algorithm is non-preemptive
    = shortest job first scheduling
        - process that have shortest next CPU burst time will get the CPU(shortest-next-CPU-burst)
        - if the next CPU burst time is equal for 2 processes, FCFS scheduling is used as a solution
        - difficulty of SJF algorithm is to identify the length of the next CPU burst 
        - SJF is used frequently in long term scheduling
        - in batch systems user specifies the process time limit when he submit the job
        - in short term scheduling there is no way to know the length of the next CPU burst 
        - next CPU burst time is generally predicted as an 'exponential average' of the measured lengths of previous CPU bursts
        - estimate[ i + 1 ] = alpha * burst[ i ] + ( 1.0 - alpha ) * estimate[ i ]  
        - SJF algorithm can be either preemptive or non-preemptive
        - preemptive SJF scheduling is sometimes called shortest-remaining-time-first scheduling 
    = priority scheduling 
        - a priority is associated with every process 
        - CPU will be allocated to the process with highest priority
        - equal priority processes are scheduled in FCFS order
        - SJF is a special case of the general priority scheduling algorithm (process with larger next burst time will have the least priority and vice versa)
        - there is no common understanding that a lower number means a higher or lower priority
        - priority scheduling algorithm can be either preemptive or non-preemptive  
        - when a process enters the ready queue with a higher priority its priority is compared to the priority of the currently running process
            . a preemptive priority scheduling algorithm will preempt the currently running process
            . a non-preemptive priority scheduling algorithm will continue with the currently running process
        - major problem with priority scheduling is 'indefinite blocking' or 'starvation'. a process in the ready queue but not getting the CPU 
        - a steady stream of higher priority processes can prevent a low priority process from ever getting the CPU 
        - solution for starvation is 'aging'(gradually increasing the priority of the processes that wait in the system for a long time) 
    = round robin scheduling 
        - RR scheduling algorithm is designed for time sharing systems 
        - RR scheduling algorithm is similar to FCFS but preemption is added to enable the system to switch between processes 
        - a small unit of time is defined 'time quantum' or 'time slice', generally from 10 to 100 milliseconds in length 
        - new process will attach to the tail of ready queue
        - CPU schedular will choose the first process in the ready queue 
        - set a timer to interrupt after 1 time quantum
        - dispatches the process 
        - if the CPU burst 
            . is 1 time quantum, the process voluntarily release the CPU
            . is more than 1 time quantum, the timer will goes off and this will cause an interrupt to the OS, context switch will be executed and the process will be put at the tail of the ready queue 
        -  round robin scheduling algorithm is preemptive
        - most modern systems have
            . time quanta ranging from 10 to 100 milliseconds 
            . context switch time is less than 10 microseconds 
        - a rule of thumb is that 80% of the CPU burst should be shorter than the time quantum    
    = multilevel queue scheduling 
        - we can use multilevel queue scheduling when we can group/level processes like foreground or background processes 
        - multilevel queue scheduling algorithm partitions the ready queue in to several separate queues
        - each process will assign to a queue based on some property of the process, such as memory size, process priority or process type 
        - each queue has its own scheduling algorithm, for eg, foreground queue is scheduled with RR algorithm and background queue with FCFS algorithm 
        - in addition, there must be scheduling algorithm among queues, for eg, fixed priority preemptive scheduling  
        - list of 5 queues in order of priority
            1. system processes (highest priority)
            2. interactive processes 
            3. interactive editing processes 
            4. batch processes 
            5. student processes (lowest priority)
        - a lower priority queue process will run only if all the above queue processes have completed
        - preemption of low priority process will happen when a higher priority process enters the ready queue 
        - algorithm provides a certain portion of time slice among queues
            . 80% of time for foreground processes, RR algorithm will use among its processes
            . 20% time for background processes , FCFS algorithm will use among its processes    
    = multilevel feedback queue scheduling 
        - most general CPU scheduling algorithm and even most complex
        - multilevel feedback queue scheduling algorithm bring some flexibility on multilevel queue scheduling algorithm
        - multilevel feedback queue scheduling algorithm allows a process to move between queues for a variety of reasons
        - eg, if a process uses too much of CPU time if a process wait too long in the lower priority queue or time quanta exceeds 
        - this algorithm defined by following parameters 
            . the number of queues
            . scheduling algorithm for each queue
            . method used to determine when to upgrade a process to a higher priority queue 
            . method used to determine when to demote a process to a lower priority queue 
            . method used to determine which queue a process will enter when that process needs service  
# thread scheduling
    = kernel is not aware of existence of threads but only process
    = kernel picks a process and allocates to the CPU for certain time quantum
    = threads are created in run time library
    = 'run time system' picks up a thread in case of user level threads 
        <img src="images/6.2.userlevelthreadscheduling.jpg" alt="" width="35%" height="9%">
        - only threads with in a process can be selected, it is called process contention scope(PCS) or process local scheduling
        - selected thread will run until for IO or process schedular context switches
        - no clock interrupt in runtime system 
        - if a thread finishes with in the quantum then start with another thread 
        - if a thread is blocked for IO, entire process will get blocked    
    = 'kernel' picks up a thread in case of kernel level threads 
        <img src="images/6.3.kernellevelthreadscheduling.jpg" alt="" width="35%" height="9%">
        - any threads with in the system can be selected it is called system contention scope(SCS) or system global scheduling
        - doing thread scheduling at kernel level requires full context switch
        - slower than the user level thread switching 
        - if a thread is blocked for IO, entire process won't be blocked(can switch to another thread in the same process)
# multiple-processor scheduling
    = approaches to multiple processor scheduling
        - asymmetric multiprocessing
            . one master processor and others are slaves
            . one processor(master processor) takes all scheduling decisions, IO processing and other system related activities
            . other processors(salve processors) execute user code
            . this asymmetric multiprocessing is simple because only one processor access the system data structures and reduces the need of data sharing
        - symmetric multiprocessing(SMP)
            . each processors are self scheduling 
            . single common ready queue or private queue for every processors 
            . virtually all modern OSs support SMP 
    = processor affinity
        - most SMP systems try to avoid migration of processes from one processor to another due to the high cost of invalidating and repopulating of data in caches
        - processor affinity, most SMP systems attempt to keep a process running on the same processor 
        - process will have an affinity for the processor in which it is currently running 
        - 2 forms of processor affinity 
            - soft affinity, OS will attempt to keep a process on a single processor, but it is possible to for a process to migrate between processors  
            - hard affinity, OS will force the process to run on a subset of processors
        - many systems provide both forms of processor affinity  
    = load balancing 
        - on SMP systems its very important to have the workload balanced among all the processors 
        - load balancing attempts to keep the workload evenly distributed across all processors in an SMP system 
        - load balancing is 
            . important on systems where each processor has its own private queue of processes to execute 
            . not important on systems with common queue, these systems will choose a runnable process immediately from the common ready queue 
        - contemporary OS support SMP and each processor does have a private queue of eligible processes - 
        - 2 general approaches of load balancing
            . push migration, a specific task periodically checks the load on each processor and if it finds an imbalance evenly distributes the load by moving(pushing) 
                              processes from overloaded to idle or less busy processors
            . pull migration, occurs when an idle processor pulls a waiting task from a busy processor
        - Linux schedular and ULE schedular(for FreeBSD) implements both techniques 
        - load balancing often counteracts the advantages of processor affinity
    = multicore processors 
        - multicore, multiple processor cores on the same physical chip
        - OS considers every core as separate physical processor
        - multicore processors are faster and consume less power than systems in which each processor has its own physical chip 
        - memory stall, the waiting period for data to become available after the processor access memory
        <img src="images/6.4.memorystall.jpg" alt="" width="25%" height="9%">
        - processor can wait 50% of its time waiting for data to become available from memory 
        - as a remedy of the above situation, recent processor designs have implemented multithreaded processor cores(multiple hardware threads assigned to each core)
        - one thread stalls the core can switch to another thread
        <img src="images/6.5.multithreadedmulticorewithmemorystall.jpg" alt="" width="25%" height="9%">
        - from OS perspective each hardware thread appears as a logical processor that is available to run a software thread
        - the ultraSPARC T3 CPU has 16 cores per chip & 8 hardware threads per core = 128 logical processors
        <img src="images/6.6.multithreadedmulticorehwcoreandswcore.jpg" alt="" width="10%" height="9%">
        - 2 ways to multithread a processing core
            . coarse grained multithreading, a thread execute on a processor until a long-latency event occurs(memory read) then switch to another thread to begin execution 
            . fine grained multithreading(interleaved), switching between threads at a much finer level of granularity. the processor architectural 
              design includes logic for thread switching, as a result the cost of switching is very small 
        - multithreaded multicore processor requires 2 different levels of scheduling 
            . decide on which software thread to run on each hardware thread(logical processor). eg, algorithm RR,FCFS, etc
            . decide on which hardware thread to run on each core
                * ultraSPARC T3 CPU used RR algorithm to schedule the eight hardware threads to each core
                * Intel Itanium(dual core with 2 hardware managed threads per core), each hardware threads assigned with a hardware urgency value ranging from 0 to 7, 0 lowest.
                  it has 5 different events that may trigger a thread switch according to the highest urgency value
# real time cpu scheduling
    = feature of a real time operating system is to respond immediately to a real time process as soon as that process requires the CPU
    = this type of OS is guaranteed to perform critical tasks on a defined time constraints
    = real time systems are event based
    = switching and processing of tasks
        - real time systems switches tasks based on priority
        - time sharing systems switches tasks based on clock interrupts 
    = 2 types of real time systems are 
        - soft real time systems, no guarantee as to when the critical process will be scheduled. only guarantee that critical tasks will get preference. 
        - hard real time systems, guarantee that the process will be serviced by its deadline 
    = minimizing latency
        - event latency, is the amount of time that elapse from when an event occur to when it is serviced 
        - 2 types of latencies affect the performance of real time systems are 
            . interrupt latency, period of time from the arrival of an interrupt at the CPU to the start of the interrupt service routine 
            . dispatch latency, the amount of time scheduling dispatcher to stop one process and start another 
        - OS need to minimize interrupt and dispatch latency to ensure that real time systems receive immediate attention   
    = periodic processes, process require the CPU at constant intervals(periods)
    = certain characteristics of the periodic processes 
        - once periodic process acquires CPU it has fixed processing time 't' 
        - a deadline 'd' by which it has to service by CPU 
        - a total period p of processing 
        - it can be expressed as 0 <= t <= d <= p 
    = priority based scheduling 
        - priority with preemption 
        - windows has 32 different priority levels and priority values of 16 to 31 are reserved for real time processes 
        - providing preemptive priority based algorithm only guarantees soft real time functionality 
        - in this scheduling 
            . process has to announce the deadline requirements to the schedular 
            . use the technique known as admission control algorithm
                * accept/admit the process for execution with a guarantee that to complete on time 
                * reject the request if it can't guarantee that the task will be serviced by its deadline 
    = rate monotonic scheduling 
        - static priority with preemption
        - if a lower priority process is running and a higher priority process becomes available to run, it will preempt the lower priority process 
        - higher priority task will get CPU more often 
        - on entering the system, each periodic task is assigned a priority inversely based on its duration
            . shorter period will assign a higher priority 
            . longer period will assign a lower priority
        - this algorithm assumes that the processing time of a period process is the same for each CPU burst  
    = earliest deadline first scheduling 
        - dynamically assigns priorities according to deadline
        - priority assignment 
            . earlier the deadline the higher the priority 
            . later the deadline the lower the priority
        - the OS must announce the deadline requirements of the new runnable process
        - process priority may have to be adjusted to reflect the deadline of the newly runnable process 
    = proportional share scheduling 
        - apportioning CPU time between applications
            . allocating T shares among all applications 
            . an application receives N shares of time 
        - T = 100 and processes A = 50 shares, B = 15 shares, C = 20 shares, (50+15+20) 85 shares allocated out of 100
        - if a new process D request 30 shares the admission controller will deny the entry to system
        - proportional share scheduling work in conjunction with an admission control 
    = POSIX real time scheduling
        - POSIX standard also provides extensions for real time computing, POSIX.1b 
        - 2 POSIX classes are
            . SCHED_FIFO
            . SCHED_RR  
        - SCHED_FIFO, schedule threads according to a first come first serve policy, no time slicing among threads of equal priority. highest priority thread at front will run until it terminates or blocks 
        - SCHED_RR, round robin policy, it provides time slicing among threads of equal priority   
# operating-system examples
# algorithm evaluation or comparing scheduling algorithm 
    = deterministic modeling 
        - this evaluation method takes a predetermined workload value and evaluates each algorithm using that workload
        - CPU burst time can be a workload 
        - processes P1,P2,P3,P4 and CPU burst time 10,29,3,7 and quantum 10 milliseconds 
        - average waiting time for 
            . FCFS(non-preemptive)  = (0+10+39+42)/4 = 22.75
            . SJF(non-preemptive)   = (10+20+0+3)/4 = 8.25
            . RR(preemptive)        = (0+32+20+23)/4 = 18.75
        - deterministic modeling is possible only if the processes are not varying much  
    = queueing model
        - queueing model or queue theory 
        - queueing model will have a queue and a server 
        - by knowing arrival rate and service rate of a process we can compute utilization, average queue length, average wait time, etc 
        - we can extract following 3 values from queue 
            . n the average queue length 
            . W average waiting time in the queue 
            . λ lambda, average arrival time of process in the queue 
        - using little's formula, we can compute one of the 3 variables if we know the other 2
            . n = λ * W 
        - CPU is a server and queue is processes which are to be served  
            . CPU has ready queue 
            . IO system has device queue 
    = simulations 
        - collect data from real processes on real machine (trace data) 
        - simulator has a variable representing a clock, as this variable increases the simulator modifies the trace data 
        - simulation can take long time to run
        - collected data can be used for comparing scheduling algorithm  
    = implementation 
        - completely accurate way to compare scheduling algorithm is to implement them on real machines
        - but this approach is very costly  
            . expense on coding the algorithm
            . modify the OS 
            . user will not like an OS which is always changing 
# extra
    = CPU scheduler will choose a process from ready queue
    <img id="myImg" src="images/6.1.cpuschedular.jpg" alt="CPU schedular" style="width:100%;max-width:100px">
    <!-- The Modal -->
    <div id="myModal" class="modal">
        <span class="close">&times;</span>
        <img class="modal-content" id="img01">
        <div id="caption"></div>
    </div> 
    <script>
        // Get the modal
        var modal = document.getElementById("myModal");
        
        // Get the image and insert it inside the modal - use its "alt" text as a caption
        var img = document.getElementById("myImg");
        var modalImg = document.getElementById("img01");
        var captionText = document.getElementById("caption");
        img.onclick = function(){
            modal.style.display = "block";
            modalImg.src = this.src;
            captionText.innerHTML = this.alt;
        }
        
        // Get the <span> element that closes the modal
        var span = document.getElementsByClassName("close")[0];
        
        // When the user clicks on <span> (x), close the modal
        span.onclick = function() { 
            modal.style.display = "none";
        }
    </script>
    = dispatcher is the module that gives control of the CPU to the process selected by the short term scheduler
    = 2 scheduling methods are
        - preemptive, when a higher priority process reaches the ready queue, the currently running process moves out from the CPU and the higher priority process gets the CPU cycle 
        - non preemptive, once the process is allocated it will hold on to the CPU till it gets terminated or moved to waiting state
    <img id="myImg" src="images/6.e.scheduling methods.jpg" alt="scheduling methods" style="width:100%;max-width:100px">
    = timer, main task of a timer is to interrupt the CPU after a specific period of time
    = eg, working of timer 
        - CPU will have a hardware timer that fires an interrupt every X milliseconds  
        - OS will initialize a counter with the amount of time(X) that a program is allowed to run
        - for every clock tick the counter is decremented by 1 
        - once the counter becomes negative, timer interrupt fires and interrupt handler will make the CPU start executing another process(or thread) 
    = process, is a running picture of a program or running instance of a program
    = thread, a process contain multiple flow of execution called multi threads
    = for a linux kernel there is no concept of thread. each thread is viewed by kernel as a separate process 
    = LWP are threads in the user space that is an interface for user level thread to kernel level thread
    <img src="images/6.e.LWP.jpg" alt="" width="7%" height="9%">
    = NUMA,non uniform memory access, CPU will have faster access to some part of main memory than to other parts 
    <img src="images/6.e.numaandcpuscheduling.jpg" alt="" width="20%" height="9%">
    __________removed lines from PCS and SCS
    = kernel level CPU scheduling algorithms will decide which process in the ready queue allocate to the CPU
    = scheduling of threads involves in 2 levels
        - scheduling of user level threads to kernel level threads via lightweight process
        - scheduling of kernel level threads by the system scheduler
    = 2 controls needs to be specified for user level threads
        - contention scope (competition among the user level thread to access kernel level threads)
        - allocation domain
    = contention scope can be classified as 
        - process contention scope(PCS) or process local scheduling
            . contention among threads with in the same process
            . highest priority thread to access the kernel via LWP 
            . application developer defines the priority
        - system contention scope(SCS) or system global scheduling
            . contention among all the threads in the system
            . one to one model only uses SCS
    = to run on CPU user level threads are mapped to a kernel level thread or light weight process(LWP) directly or indirectly 
    = kernel level threads are being scheduled by OS
    = user level threads are managed by thread library 
    = contention scope refers to the scope in which threads compete for the use of physical CPUs 
    = 2 basic ways of scheduling 'threads' in OS are  
        - process contention scope(PCS) or process local scheduling
            . thread library uses PCS to schedules user level threads to run on an available LWP
            . the contention takes place among the threads belongs to the same process 
            . PCS is done according to the priority, user level threads priority is set by the programmer 
            . on many to one and many to many models uses PCS 
        - system contention scope(SCS) or system global scheduling
            . kernel uses SCS to decide which kernel thread uses LWP to get the CPU
            . the contention for CPU is among all threads on the system 
            . one to one model only uses SCS
    = Pthread scheduling
        -  POSIX provide API that allows PCS or SCS during thread creation 
            . PTHREAD_SCOPE_PROCESS for PCS scheduling
            . PTHREAD_SCOPE_SYSTEM for SCS scheduling 
<!-- </details>   -->
<hr>
<b id="id_deadlocks_detail">deadlocks</b>
# introduction
    = a situation in which 2 processes sharing the same resource are effectively preventing each other from accessing the resources 
    <img src="images/7.1.deadlock.jpg" alt="" width="9%" height="9%">      <img src="images/7.2kansasfoolishlaw.jpg" alt="kansas foolish law" width="14%" height="13%">
    = process P1 requesting R2 with out releasing R1, realizing that P1 complete with R1
    = process P2 requesting R1 with out releasing R2, realizing that P2 complete with R2
    = deadlock, resources which has requested by a process are held by other waiting processes
    = OS typically do not provide deadlock prevention facilities, it remains the responsibility of programmers to ensure that they design deadlock free programs
# system model
    = system resources are
        - physical resource 
            . CPU cycles
            . IO devices 
        - logical resource 
            . files
            . semaphore and mutex locks 
    = system table, OS will record resource status(free or allocated) and a related process in a system table 
    = resource waiting queue, if a process requests a resource that is currently allocated then this process will be added to the queue of waiting for the same resource 
# deadlock characterization
    = necessary conditions 
        - a deadlock can occur if the following 4 conditions exist simultaneously, avoid any of the condition to prevent deadlock  
            . mutual exclusion, at least one non sharable resource 
            . hold and wait, holding one resource and waiting for another  
            . no preemption, the process will not release the resource voluntarily for another process 
            . circular wait, P1 waiting for R2 which is allocated to P2 and P2 waiting for R1 which is allocated to P1
            <img src="images/7.1.deadlock.jpg" alt="" width="9%" height="9%"> 
    = resource allocation graph 
        - dead lock can be described more precisely with a graph called resource allocation graph 
        - how process utilize a resource
            . request, if the requested resource is not available immediately then the process must wait until it acquire the resource 
            . use, process operate on resource 
            . release, process releases the resource 
        - various kinds of request and releases 
            . devices - request() and release()
            . file - open() and close()
            . memory system calls - allocate and free()
            . semaphore - wait() and signal()
            . mutex lock - acquire() and release()
        - P to R is request and R to p is allocation
        <img src="images/7.3.resourceallocationgraph.jpg" alt="" width="9%" height="9%"> 
        - if graph contains no cycle
            . no deadlock
        - if graph contains cycle 
            . if one instance per resource type then deadlock will occur
            . if several instances per resource type then possibility of deadlock
# methods for handling deadlocks
    = use a protocol to prevent or avoid deadlocks 
    = allow the system to enter a deadlocked state, detect it and recover 
    = ignore the problem and pretend that deadlock never occur in the system (most used by linux and windows, upto the application programmer to handle the deadlock)
# deadlock prevention
    = a deadlock can occur if all the following conditions are true 
        - if none of the resources can be shared (mutual exclusion)
        - if a process can hold a resource and wait for other resource (hold and wait) 
        - if a resource is allocated then it can't be preempted (no preemption) 
        - if a process can hold a resource and wait for other resource (circular wait)  
    = in order to prevent a deadlock we need to make any of the above conditions to false 
    = mutual exclusion
        - make at least one resource sharable to avoid(false) mutual exclusion
        - this can be done on a case-by-case basis
            . a read-only file can be made a shareable resource 
            . a dvd writer can't be made a shareable resource
    = hold and wait
        - avoid holding one resource but waiting for another resource to start execution
        - 2 methods which can be implemented 
            . each processes to request all the required resources before it begins execution 
            . allow process to request more resources only after releasing all the resources that it is currently allocated  
        - disadvantages are 
            . in first method, resource utilization will be very low since resource will be allocated but not used for long period
            . in second method, starvation is possible in case of popular resources
    = no preemption
        - allow process/resource to have preemption 
        - probable method 1
            . time quantum for each resource
            . the running process will preempt the resource after a time quantum 
        - probable method 2 
            . if a process is currently holding some resources and requests for some more resource, which cannot be allocated immediately
                * the process will preempt 
                * currently holding resource and requested resource will be populated in to the waiting resource list 
                * the process restarts only when all resources are available 
        - probable method 3
            . if a process request some resources, which cannot be allocated immediately
                * if a resource is allocated to a process in the wait queue, the resource is preempted from the waiting process and allocated to the requesting process
                * If the resource is not available and is not blocked in a waiting queue, the requesting process must wait 
            . this method is often applied to resources whose state can be easily saved and restored later such as CPU registers and memory space 
    = circular wait 
        - to avoid circular wait 
            . number each resources
            . provision to request resources only in ascending order
# deadlock avoidance
    = in order to avoid deadlock, we need to identify a 'safe sequence' of processes and we need to make sure that system never enters in to an unsafe state
    = one way to avoid deadlocks is to get additional information about how resources
        - currently available resources
        - currently allocated resources to every process 
        - future requests and releases of each process 
    = 2 deadlock avoidance algorithms
        - safe state
            . a state is safe if the system can allocate resources to each process in some order and still avoid a deadlock 
            . a system is in a safe state only if there exists a 'safe sequence' to allocate resources
            . the system can be in unsafe state if there is a possibility of deadlock. system won't be able to fulfill the request of all processes
        - resource allocation graph algorithm
            . if only one instance of resource then a variant of resource allocation graph can be applied to avoid deadlock 
            . edges in graph 
                * assignment edge(resource to process)
                * request edge(process to resource)
                * claim edge (dotted line, process to resource)
            . before the start of process, all the edges must appear in the graph 
            . this algorithm make sure that the cycle formation is avoided, so that deadlock wont occur 
        - banker's algorithm (safety algorithm) 
            . if resource having multiple instances then we can use bankers algorithm
            . banker's algorithm is a resource allocation and deadlock avoidance algorithm, determining whether requests can be safely granted 
            . it will check if allocation of any resource will lead to deadlock and build a 'safe sequence' we are trying to build a 'safe sequence' of processes  
            . 'safe sequence' is a sequence of processes where the resource allocation can happen with out a deadlock
            . predetermined matrix are 
                * total(totally available resources)
                * max(maximum resource requirement per process)
                * allocated(currently allocated resources per process)
                * work(available,currently freely available resources)
                * need(additional resources required per process) 
            . maximum requirement of resources for any process should not be more than the total resources
            . step 1 populate predetermined matrix values
            . step 2 calculate and modify need value(maximum - allocated) for all the process 
            . step 3 loop through all the processes to identify the 'safe sequence'
                * need <= work
                * if its true then update free resources(work+allocated) 
                * if its false select next process 
                * cycle through all the process until all the processes are included in the 'safe sequence' list 
            . illustration https://www.youtube.com/watch?v=d_Kr-Z-UzEg
        - banker's algorithm (resource request algorithm) 
            . predetermined matrix are similar for both banker's algorithm
            . step 1 populate predetermined matrix values
            . step 2 calculate and modify values(allocation, available,need) only for the process which is requested  
                * 1 if request <= need, goto step 2, otherwise raise an error 
                * 2 if request <= available goto step 3, otherwise process must wait 
                * 3 modify the matrix according to the formula  
                    available = available - request
                    allocation = allocation + request 
                    need = need - request 
            . step 3 loop through all the processes to identify the 'safe sequence'(similar as banker's algorithm safety algorithm)
            . illustration https://www.youtube.com/watch?v=GUnNgMgQlGo
# deadlock detection
    = if the system doesn't employee deadlock prevention or deadlock avoidance algorithm then there is a possibility of having deadlock on processes, then
        - the system has to examine whether a deadlock has occurred
        - if yes, implement an algorithm to recover from deadlock
    = different detections methods are 
        - if the resource is only having a single instance, then use a variant of 'resource allocation graph' ie 'wait for graph'
        - if the resource is having multiple instances, then use kind of 'bankers algorithm'
    = single instance of each resource type 
        - algorithm uses a variant of resource allocation graph which is 'wait for graph' to detect a deadlock 
        - connect processes by removing resource nodes on a resource allocation graph if one of the process is requesting a resource and same resource is allocated to another process
        - a deadlock can be detected, if a cycle is formed on a 'wait for graph'
        - the system needs to maintain the wait for graph and periodically invoke an algorithm that searches for a cycle in the graph 
        - illustration https://www.youtube.com/watch?v=ad2BXy21DLs&list=PLTwXRHDoo-A8NY12YjOXNIx7YpGXNKfyG&index=6
    = several instances of a resources type 
        - we need to use kind of 'bankers algorithm' to detect for deadlock
        - the data structures used for this algorithm
            . available(work), number of available resources of each system  
            . allocation, currently allocated to each process 
            . request, current request of each process 
        - illustration https://www.youtube.com/watch?v=ad2BXy21DLs&list=PLTwXRHDoo-A8NY12YjOXNIx7YpGXNKfyG&index=6
    = detection algorithm usage(when should we invoke)
        - invoke the detection algorithm for every resource request 
        - invoke the detection algorithm at defined intervals ie, once every hour or when CPU becomes 40%
# recovery from deadlock
    = 2 recovery mechanisms once we detect a deadlock 
        - process termination 
        - resource preemption 
    = process termination 
        - abort all deadlocked processes 
        - abort one process at a time until the deadlock cycle is eliminated
            . 1 abort a process 
            . 2 apply deadlock detection algorithm
            . if deadlock cycle detected, start from step 1 else process continues its execution   
    = resource preemption
        - selecting a victim(process/resource)
        - rollback, once a resource is preempted from a process, the process can't continue the normal operation, better to do a total rollback
        - starvation, if a process is selected as a victim multiple times then starvation can happen. we must ensure that a process can be picked as a victim only a finite time 
<!-- </details> -->
<hr>
<b id="id_mainmemory_detail">main memory</b>
# background
    = main purpose of multitasking or time sharing is to make sure that the CPU is always busy
    = CPU can be shared by several processes
    = these processes must be kept in memory in order to keep CPU always busy
    = objectives 
        - various ways of organizing memory hardware 
        - techniques of allocating memory to processes 
        - how paging works in contemporary computer systems 
    = how memory works
        - memory consists of a large array of bytes each with its own address 
        - CPU fetches instructions from memory according to the value of the program counter 
        - these instructions may cause additional loading from and storing to specific memory addresses 
    = a typical instruction execution cycle 
        - fetch an instruction from memory 
        - decode the instruction, it may cause operands to be fetched from memory 
        - after the execution the results may be stored back in memory 
    = basic memory related hardware 
        - cache 
            . general purpose storage that a CPU can access directly(direct access storage devices)
                * main memory
                * registers built in to the processors 
            . machine instructions takes only memory address not disk address as an argument
            . so the instructions or data must be available in one of these direct access storage devices 
            . if instructions or data is not available in memory they must be moved there before CPU can operate on them 
            . registers built in CPU are generally accessible within one cycle of the CPU clock 
            . access time difference
                * at the rate of one or more operations per clock ticks on register contents 
                * many cycles of the CPU clock ticks for main memory access 
            . main memory is accessed through memory bus which will stall(delay) the processor
            . to speed up accessing physical memory, add fast memory(cache) between CPU and main memory to avoid delay 
            . basically cache will speed up physical memory access 
        - protection 
            . make sure that user processes do not access the OS
            . on multiuser systems, make sure that user process do not access each other
            . this protection must be provided by the hardware because the OS wont intervene between the CPU and its memory accesses. implementation through 2 registers
                * base register(smallest physical memory address)
                * limit registers(size of the range)
            <img src="images/8.1.baseandlimitregisters.jpg" alt="" width="10%" height="9%">
            . the logical address space of this process consists of 120,899 address positions from the 300,040th address. the last will be 420,939(inclusive)th address 
            . how base and limit registers loaded 
                * special privileged instructions will be used to load registers 
                * these instructions have the permission to execute only in kernel mode
                * only OS can execute in kernel mode 
            . protection from accessing an illegal address by a process is handled by CPU hardware 
            . CPU hardware compares every address generated in user mode with these registers 
            <img src="images/8.2.ostrapaddressingerror.jpg" alt="" width="25%" height="9%">
            . any illegal attempt will be results as a trap to the OS which treat it as a fatal error 
    = address binding 
        - a program resides on a disk as a binary executable file 
        - to be executed, the program must be brought into memory and placed within a process 
        - the process may move between disk and memory during its execution
        - processes on disk waiting to be brought into memory for execution in the form of 'input queue'
        - a single-task procedure will select a process from the queue and load in to memory 
        - as the process is executed, it accesses instructions and data from memory 
        - eventually the process terminates and its memory space is declared available 
        - most systems allow a user process to reside in any part of the physical memory 
        - physical memory starts from 0 but first address of a user process need not be 0 
        - different kind of addresses till execution 
            . symbolic address, addresses in the source program are generally symbolic(variable count)
            . relocatable address, compiler typically binds these symbolic addresses to relocatable addresses(32 bytes from the beginning of this module)
            . absolute address, linkage editor or loader will binds the relocatable addresses to absolute addresses(such as 74014)
        - each binding is a mapping from one address space to another 
        - binding of instructions and data to memory addresses done at following ways 
            . compile time, you can generate 'absolute code' at compile time if you know where the process will reside in memory. MS-DOS's .COM format is bound at compile time 
            . load time, if 'absolute code' can't be generated at compile time then the compiler must generate 'relocatable code'. final binding will be delayed until load time
            . execution(run) time, if the process can be moved from one memory segment to another, then the binding must be delayed till run time. most general purpose OS use this method 
            <img src="images/8.3.bindingofinstructionsanddatatomemoryaddresses.jpg" alt="" width="9%" height="9%">
    = logical versus physical address space 
        - an address generated by the CPU is commonly referred to as a 'logical address' 
        - logical address generated by 'execution time binding' scheme is 'virtual address'
        - actual location in RAM is called 'physical address'
        - logical address space, set of all logical addresses generated by a program 
        - physical address space, set of all physical addresses corresponding to these logical addresses   
        - MMU (Memory Management Unit), execution time binding is done by a hardware device called MMU
        - MMU uses relocation(base) register for mapping the physical address. the value of the relocation register is added to every address generated by a user process
            . user program never sees the real physical addresses, user program deals with logical addresses only
            . hardware converts the logical address to physical address this form is called execution time binding 
        <img src="images/8.4.dynamicrelocationusingrelocationregister.jpg" alt="" width="10%" height="9%">
    = dynamic loading 
        - keeping the entire instructions and data in memory is literally difficult since it will limit the process to the size of physical memory
        - static linking, system libraries are treated like any other object module and are combined by loader into the binary program image 
        - dynamic loading
            . a routine is not loaded until it is called
            . all routines are kept in disk in a relocatable load format 
            . main program is loaded in to memory and begins the execution 
            . when a routine needs to call another routine, relocatable linking loader will load the desired routine into memory and update program's address tables
            . programmer to design their programs to take advantage of such a method 
    = dynamic linking and shared libraries 
        - loading, load the program from secondary memory to main memory
        - linking, generating an executable program by combining all modules/function of a program
            . linking loader, performs linking and relocation operation and loads the linked program directly into memory for execution  
            . linkage editor, produces a linked version of the program prior to load time 
        - static linking, linker copying all library routines used in the program into an big executable image
        - dynamic linking, linking operation is postponed till execution time  
        - an advantage of dynamic linking is that only one copy of these libraries (DLL/shared library/shared object) exists in memory
# swapping
    = a process must be loaded in to memory in order to execute 
    = many processes are required to be in memory for multiprogramming 
    = all processes can't reside in memory since physical memory is limited
    = a process which is currently not using can be moved out of memory to a fast disk(backing store) and brought back into memory when required  
    = modern OS no longer uses swapping because it is too slow and there faster alternatives are available(like paging) 
    = standard swapping
        - involves in moving processes between main memory and a backing store 
        - backing store must me fast disk
        - disk should have enough capacity to hold copies of all memory images for all users 
        - system maintains a ready queue, which contains all processes in the backing store and only processes in memory that are in the 'ready to run' state
        - whenever CPU schedular decides to execute a process, it calls the dispatcher 
        - dispatcher checks the queue if the next process is in memory, if not 
            . dispatcher checks if there is free memory region, if not
                * dispatcher swaps out a process and swaps in the desired process and reloads registers(context-switch) and transfers control to the selected process        
        - swap only if the process is completely idle
        - if a process is swapped which was pending for IO, the IO results may be written to memory which belong to another process 
        - to resolve the above issue 
            . execute IO operations to OS buffers 
            . then transfer from OS buffer to process memory after the process is swapped in (double buffering)
        - standard swapping is not used in modern OS
        - modified version of swapping found in many systems 
            . swaps only if amount of free memory falls below a threshold amount 
            . swapping only a portion of process rather than entire process 
    = swapping on mobile systems 
        - mobile systems never support swapping in any form 
        - apple IOS 
            . asks applications to voluntarily free up memory 
            . read only data simply removed 
            . modified data, like stack, never removed
            . OS will remove data if any application failed to remove it 
        - android
            . follows similar strategy 
            . but writes application state to flash memory for quick restarting 
# contiguous memory allocation
    = main memory is usually divided in to 2 partitions to accommodate OS and various user processes 
    = we can place OS in low or high memory based on the interrupt vector location, usually interrupt vector is in low memory 
    = memory allocation is allocating available memory to the processes that are in the input queue waiting to be brought into memory 
    = contiguous memory allocation, each process is contained in a single section of memory that is contiguous to the section containing the next process
    = memory protection  
        - protection from illegal access. this mechanism will prevent running process from accessing memory locations it doesn't own
        - workflow of protection
            . CPU scheduler selects a process for execution
            . the dispatcher loads the relocation and limit register as part of context switch
            . every logical address generated by a CPU will be checked against these 2 registers  
        - through this method OS and the other users programs and data can be protected from illegal access by running program 
        - relocation register scheme provide an effective way to allow the OS to change its size dynamically
        <img src="images/8.5.hwsupportforrelocationandlimitregisters..jpg" alt="" width="13%" height="9%">
        - transient code will comes and goes from memory as needed, this will change the size of OS. like device driver code which is not commonly in used 
    = memory allocation 
        - multi partition method
            . divide memory into several fixed sized partitions and each partition contain a process. every partition will be refilled from input queue when one become free. this is no longer used
        - variable partition method 
            . OS keeps a table indicating which parts of memory are available(hole) and are occupied 
            . OS allocates memory to a process based on the memory requirements of each process and the amount of available memory space 
            . various memory allocation strategies to find the best hole for allocation are 
                - first fit, allocate to the first hole with enough space. search can start from the beginning or from where the last search ended. the search ends as soon as the free hole is found
                - best fit, allocate to the smallest hole that is big enough. needs to search the entire list. disadvantage is, unused portion of the hole may be very small to be used for another process 
                - worst fit, allocate to the largest hole available. needs to search the entire list. unused portion of the hole may be large enough to be used for another process 
            . simulation shown that 
                - first or best fit is better than worst fit in terms of time and storage utilization 
                - neither first or best fit is better than the other in terms of storage utilization 
                - first fit is better in terms of time         
    = fragmentation 
        - free memory space gets fragmented, 2 types of fragmentation are 
        - internal fragmentation 
            . break the physical memory into fixed sized blocks and allocate memory in units based on block size
            . difference of unused space from the block size is called internal fragmentation 
            . all memory allocation strategies even suffer from internal fragmentation
        - external fragmentation
            . when allocation happens in a bigger memory space, holes are broken into lots of little pieces, none of which is big enough for a process but sum total could
            . all memory allocation strategies suffer from external fragmentation,first and best fit suffers more than worst fit 
            . 50 percentage rule, statical analysis of first fit reveals that 1/3 of memory will be lost in fragmentation and it will be unused this property is known as 50 percent rule 
        - solutions to external fragmentation problem
            . compaction, move all processes down to one end of physical memory and update relocation register for each process. only condition is to have relocatable code for the process in memory
            . allow logical address space of the processes to be non-contiguous, 2 complementary techniques achieve this solution   
                - segmentation 
                - paging  
# segmentation 
    = segmentation permits the physical address space of a process to be non-contiguous 
    = basic method
        - programmers tend to think their memory in multiple segments, each dedicated to a particular use such as code, data, stack, heap etc
        - logical address space is a collection of segments 
        - each segment has a name and a length. for simplicity number used instead of name 
        - a logical address consists of 2 tuples (segment-number,offset)
        - when a program is complied the compiler automatically constructs segments reflecting the input program
        - eg, C complier might generate 5 segments those are user code, library code, global(static) variables, stack, head
        - loader would take all the segments and assign them segment numbers 
    = segmentation hardware 
        - object in a program can be refer with a 2 dimensional address
        - physical address is still a 1 dimensional address 
        - segmentation table is the implementation for mapping 2D logical address to 1D physical address 
        - each entry in the segment table has a segment base and segment limit 
            . base, segment base is the starting physical address where the segment resides in the memory 
            . limit, segment limit specifies the length of the segment 
        - a logical address consist of 2 parts 
            . segment number(s), is used as an index to segment table 
            . offset(d) into that segment
        - working   
            . CPU produces a logical address(segment number,offset)
            . segment number will be used to extract the limit and base from segment table 
            . limit and offset will be compared, 
                * if the offset is a less number than limit then to the next stage 
                * if the offset is not less than limit then trigger an OS trap
            . next stage, base + offset = physical address  
        <img src="images/8.6.segmentationhardware.jpg" alt="" width="13%" height="9%"> <img src="images/8.7.segmentationexample.jpg" alt="" width="13%" height="9%">
        - segmentation won't avoids external fragmentation and the need for compaction
# paging
    = paging permits the physical address space of a process to be non-contiguous 
    = paging also avoids external fragmentation and the need for compaction 
    = it also solves fragmentation problem on backing store 
    = because of its advantages paging in its various forms is used in most OS from mainframes through smartphones
    = paging is implemented through cooperation between the OS and the hardware 
    = basic method
        - divide physical memory into fixed sized blocks called frames 
        - divide logical memory(process) into same sized blocks called pages
        - divide backing store into fixed sized blocks that are the same size as the frames or clusters of memory frames 
        - sizes of frame and page blocks needs to be same
        - when a process is to be executed, its pages will be loaded in to any frames from their sources(file system or back store)
        - logical address from CPU is a combination of page number and page offset/displacement 
        - page number will be used as the index in page table 
        - page table contains the mapping of the page number to the base address of each page in physical memory
        - base address of each page combined with page offset is the physical address that is sent to the memory unit 
        <img src="images/8.8.paginghardware.jpg" alt="" width="13%" height="9%">   <img src="images/8.9.pageingexample.jpg" alt="" width="13%" height="9%">  <img src="images/8.9.1.pageingexample.jpg" alt="" width="13%" height="9%">
        - page size like the frame size is defined by hardware 
        - size of a page is a power of 2 varying between 512(2^9) bytes and 1GB(2^30) per page depending on computer architecture
        - example 
            . if logical address space have 2^7(128) address spaces and each page size is 2^4(16)
            . high order 3 bits(m-n=7-4) designate to page number 
            . lower 4 bits(n) designate to page offset
            . so this logical address space contains 
                * 8 pages 
                * each page have 16 entries
            . each logical address is a combination of page number and page offset  
            <img src="images/8.10.logicaladdresscombinationofpage numberandpageoffset.jpg" alt="" width="9%" height="9%">    
        - obtain the page size in Linux 
            . getpagesize() system call
            . getconf PAGESIZE (command line) 
        - paging itself is a form of dynamic relocation, every logical address is bound by the paging hardware to some physical address 
        - paging avoids external fragmentation, but there is a possibility of internal fragmentation
        - currently typical page size are between 4KB and 8KB
        - some CPU and OS supports multiple page sizes. Solaris uses 8KB and 4MB 
        - workflow of populating page table 
            <img src="images/8.11.freeframesbeforeandafterallocation.jpg" alt="" width="9%" height="9%">    
            . when a process arrives in the system to be executed its page size is examined 
            . each page needs one frame, if process require n pages then n free frames required in the memory 
            . if n frames are available then the arriving process will be allocated to these frames
            . the first page is loaded into one of the allocated frame 
            . frame number is put in the page table for this process 
            . same steps follows for all the frames of this process 
        - address-translation hardware reconcile between programers view of memory(big memory for only currently running program) and physical memory where running program is scattered throughout
        - address mapping is handled by OS
        - OS is aware of the allocation details of physical memory, which frames are allocated, which frames are available, total frames and so on
        - information is kept in a data structure called 'frame table'
        - 'frame table' has one entry for each frames, indicating whether is free or allocated and to which page of which process it is allocated 
        - paging increases the context switch time 
    = hardware support(different types of hardware implementation of the page table)
        - implement page table on fast registers
            . set of dedicated registers
            . CPU dispatcher reloads these registers 
            . instructions to load and modify are privileged
            . eg, DEC PDP-11, logical address is 16 bit long, page offset(size,d) is 8 bits and page number(p) is 8 bits(16-8). all these 8 entries of page table will be kept in fast registers
            . this method is satisfactory if the page table is reasonably small(256 entries) 
        - implement page table in main memory and a PTBR(page table base) register to point to this location  
            . most contemporary computers allow page table to be very large(1 million entries)
            . for this type of systems page table will be kept in main memory 
            . a page table base register(PTBR) points to the page table in main memory 
            . context switch requires only to change a register, this will substantially reduce context switching time
            . but more time required to access the memory location, 2 memory accesses are needed to access a byte 
                * get the frame number by using page number from location address i and offset from PTBR
                * get the actual address by using the frame number and offset from PTBR 
        - implement TLB(translation look aside buffer) for mapping between CPU and page table
            . TLB ia associative, high speed memory
            . each entry in TLB consist of 2 parts
                * a key(tag)
                * a value
            . when the TLB is presented with an item, the item is compared with all keys simultaneously 
                * if item found, corresponding value field is returned
            . TLB lookup is part of the instruction pipeline so no performance penalty
            . typically TLB contains between 32 and 1024 entries in size 
            . some CPUs implement different instruction and data address TLBs, that can double the number of TLB entries
            . systems have evolved to have multiple levels of TLBs just as multiple levels of caches 
            . workflow
                * TLB contains only a few of the page table entries 
                * when CPU generates logical address, its page number part is presented to the TLB 
                * if page number 
                    ** found in TLB(TLB hit) its associated frame number is used to identify the physical address
                    ** is not found (TLB miss) 
                        *** first access memory for the page table    
                        *** missed entries will be added(page and frame number) to TLB for further reference 
                        *** if TLB is already full of entries, an existing entry must be selected for replacement through replacement policies. some CPUs allow OS to do replacement others by themselves 
                        *** management of TLB defaults automatically or via OS interrupt
            . some TLB allow certain entries to be wired down, ie they cannot be removed from the TLB like the key kernel code 
            . some TLBs store the ASID in each TLB entry and ensure that the incoming logical address request is for the current running process
            . advantage of having ASID in TLB
                * it provides address space protection 
                * allow TLB to have several different processes simultaneously else the TLB must be flushed every time context switch happens 
            . hit ratio, the percentage of times that the page number of interest is found in the TLB 
            . example(time in nanoseconds,ns)  
                * TLB hit will take 100ns to get the byte from memory 
                * TLB miss will take 200ns to get the byte from memory(100ns for the memory access to get the page table for frame number and another 100ms to get the byte from memory) 
                * if TLB hit is 80%
                    ** effective access time = 0.80*100 + 0.20*200 = 120ns  
                * if TLB hit is 99%(which is more realistic and only 1% slowdown compared to 80% hit)
                    ** effective access time = 0.99*100 + 0.01*200 = 101ns
            . CPUs today provide multiple levels of TLBs 
            . Intel Core i7 CPU has 128-entry L1 instruction TLB and 64-entry L1 data TLB 
            . change in the TLB design may necessitate a change in the paging implementation of the OS
            <img src="images/8.12.paginghardwarewithTLB.jpg" alt="" width="9%" height="9%">    
    = protection 
        - the page table can also help to protect processes from accessing memory
        - a bit or bits can be added to the page table to classify a page as read-write, read-only, read-write-execute or some combination of these sort
        - each memory reference can be checked to ensure it is accessing the memory in the appropriate mode else trap to the OS
        - one additional bit is generally attached to each entry in the page table 'valid-invalid bit'
            . when bit is valid, associated page is in the process's logical address space and it's a valid page 
            . when bit is invalid, associated page is not in the process's logical address space and it's a invalid page 
        - valid-invalid bit cannot block all illegal memory access due to internal fragmentation(last page may not entirely fill with the process, and may contain data left over from previous process)
        - one solution to the above problem is, check every logical address to a valid range for the process. range or length is stored on a page table length register(PTLR)
    = shared pages 
        - one of the advantages for organizing memory according to pages is to share the same physical pages
        - common code can be shared through paging, which will reduce common code redundancy 
        - this common code is read only and its called reentrant code (pure code) it never changes during execution 
        <img src="images/8.13.sharingpages.jpg" alt="" width="9%" height="9%"> 
        - only one copy of reentrant code needs to kept in physical memory for multiple processes 
# structure of the page table 
    = if the page table size is too large to fit in a frame, the page table must be broken to do in the frame. common structuring(breaking) techniques follows 
    = each page table entry contains 
        - frame number,f : physical frame number 
        - valid/invalid,v : whether the requested page is present or absent. valid(1)/invalid(0). if absent it will generate a page fault 
        - protection,p : protect the page based on the value like read/write/execute
        - reference,r : 0 means this page is referring for the first time, 1 means this page has used in past as well 
        - caching,c : enable or disable, do we need to keep in cache 
        - dirty,d : modify, whether the page has modified after it was swapped in 
        <img src="images/8.14.2.pagetableentry.jpg" alt="" width="9%" height="9%"> 
    = hierarchical paging 
        - example to highlight the need
            . physical address space=256MB(2^8+2^20=28bits) and frame size=4KB(2^2+2^10=12bits)
            . logical address space=4GB(2^2+2^30=32bits) and page size will be same(4KB) as frame size as rule  
            . each page table entry=2bytes
            . physical address space contains 2^16(2^28/2^12) frames 
            . now logical address can be divided in to 20+12=32bits(low 12bits represent page size and upper 20bits represent frame number)     
            . so logical address space contains 2^20(1MB, 1 million) pages 
            . address space contains 
                * physical 2^16 frames and 4KB of size
                * logical 2^20 pages and 4KB of size
            . there will be a corresponding entry for each page in the page table so 1 million entries in page table
            . page table total size is 1million(2^20) * 2bytes(2^1) = 2MB(2^21)
        - since page table size > frame size(2MB > 4KB), so page table will not fit in to a frame 
        - solution is to divide the page table in to small sizes
            . page table will be divided in to 2048pages(4KB/2bytes=2^11,2KB) with 2KB in size for each row and named as inner(page of) page table
            . create one more page table for mapping called outer page table
            . outer table will have 512 pages(2^21/2^12=2^9) and every entry of 2bytes in size and a total size of 1KB(512bytes,2^9 * 2bytes,2^1)
        - now the logical address will be read differently 9+11+12=32bits
            . 9 is the page number in outer page table 
            . 11 is the page offset in inner page table 
            . 12 is the frame number in main memory
        <img src="images/8.14.twolevelpagetablescheme.jpg" alt="" width="9%" height="9%"> <img src="images/8.15.addresstransilationfor2levelpaging.jpg" alt="" width="9%" height="9%">
        - hierarchical paging technique is also called as forward mapping page table or multilevel page table 
    = hashed page tables
        - common approach for handling address space larger than 32 bits is to use a hashed page table 
        - hash value will be used as the index to a location in hash table 
        - each entry in the hash table contains a linked list of elements that linked to a same hash value
        - each element in the linked list contains 3 fields. page number, frame number and a pointer to the next element in the linked list 
        - working
            . page number from the logical address is hashed into the hash table 
            . hash value will be indexed to an entry in the hash table, that entry contains 3 fields
            . page number will be compared with the 1st field 
                * if match, the corresponding frame(field 2) is used to form the desired physical address 
                * if no match, subsequent entries in the linked list will be searched 
        - variation of this scheme that is useful for 64 bit address space, this variation uses clustered page tables 
        - clustered page table, each entry in the hash table refers to several pages rather than a single page 
        - single entry in the hash table can store the mappings for multiple physical page frames 
        <img src="images/8.16.hasedpagetable.jpg" alt="" width="9%" height="9%">
    = inverted page tables 
        - usually each process has an associated page table and has one entry for each page 
        - drawback of that method(one process one page table) is these page tables may consume large amounts of physical memory 
        - solution for this drawback is 'inverted page table'
        - inverted page table
            . purpose of this form of page management is to reduce the amount of physical memory needed to track virtual-to-physical address translations 
            . accomplish this saving by creating a table that has one entry per page of physical memory indexed by <process-id,page-number> 
            . inverted page table stores the information of page to physical frame 
            . it is inverted which means we look at the mapping from a physical memory frame back to a virtual page, despite the fact that the translation starts from virtual address just like normal  
            . logical address in this system consists of a triple 'process id,page number,offset'
            . inverted page table is then searched for a match 
                * if found, then physical address is generated 
                * if not found, then illegal address access has been attempted 
            <img src="images/8.17.invertedpagetable.jpg" alt="" width="9%" height="9%">
# example: intel 32 and 64-bit architectures
    = IA-32 architecture
        - CPU generates logical addresses and given to the segmentation unit 
        - segmentation unit produces a linear address for each logical address and given to paging unit 
        - paging unit produces a physical address of main memory 
        <img src="images/8.18.logicaltophysicaladdresstransilationinIA32.jpg"" width="9%" height="9%">
        - thus segmentation and paging unit is equivalent of the MMU 
        - IA-32 segmentation 
            . logical address space of the process is divided in to 2 partitions of 8K each
                * 1st consists of segments that are private to that process 
                * 2nd consists of segments that are shared among all the processes
            . information about the 1st portion is kept in the local descriptor table(LDT)
            . information about the 2nd portion is kept in the global descriptor table(GDT)
            . each entry in the LDT and GDT consists of an 8byte segment descriptor 
            . segment descriptor provides the processor with data it needs to translate a logical address to linear address, data includes base and limit addresses
            . logical address is a pair of 'selector' and 'offset'and 48 bit number 
                * selector is a 16 bit number (13,1,2). 13 bit designates the segment number s, 1 bit for whether in GDT or LDT, 2 for protection 
                * offset is a 32 bit number
            <img src="images/8.19.IA32segmentation.jpg" alt="" width="9%" height="9%">
            . segment number in the selector points to the appropriate entry in the LDT or GDT 
            . limit and base information about the segment is used to generate a linear address 
            . linear address on the IA-32 is 32 bit long 
            . machine has 6 segment registers allowing 6 segments to be addressed at any one time by a process 
            . machine has 6 micro program register(8 byte), to hold corresponding descriptors from either the LDT or GDT, this cache lets the Pentium avoid having to read the descriptor from memory
            . IA-32 architecture allows a segment to be as large as 4GB and maximum number of segments per process is 16K
        - IA-32 paging 
            . IA-32 architecture allows a page size of either 4KB or 4MB
            . IA-32 uses a 2 level paging scheme in which the division of the 32bit linear address as page number and page offset 
            . IA-32 follows hierarchical paging for address translation 
            . one entry in the page directory is PAGE_SIZE flag, indicates the size of the page frame. if flag is not set means its 4KB else its 4KB
            . if page size is standard 4KB
                * linear address is a combination of page directory(10 bits), page table(10) and offset(12) 
                * 10 high order bits index in to the list of entries in the outermost page table(page directory) and this table will be pointed by CR3 register(contains current process's page directory)
                * 10 innermost bits index in to the list of entries of the inner page table(page table) and this table was indicated by the page directory entry    
                * l2 lower order bits refer to the offset in the 4KB page pointed to in the page table
            . if page size is 4MB 
                * 10 high order bits reference an entry in the outermost page table(page directory) which will be a pointer directly to the 4MB page bypassing the inner page table
                * 22 lower order bits index to the offset in the 4MB page
                <img src="images/8.20.paginginIA32architecture.jpg" alt="" width="9%" height="9%">
            . IA-32 'page tables' can be swapped to disk
            . an 'invalid bit' is used in the page directory entry to indicate whether the page table is in disk or in memory
            . if the table is in disk, OS can use the remaining 31 bits to identify the location on the disk 
            . 32 bit architecture can only access 4 GB of memory addresses, but this limitation was overcome by 'Page Address Extension(PAE)' and Intel adopted this technology
            . Page Address Extension(PAE)
                * it is a 3 level scheme instead of 2 level where top 2 bits refer to a 'page directory pointer table'
                * PAE system supports upto 2MB pages 
                * PAE also increased the page directory and page table entries from 32 to 64 bits in size, 
                * which allowed the base address of page tables and page frames to extend from 20 to 24 bits and offset is 12 bits 
                  <img src="images/8.21.pageaddressextensions.jpg" alt="" width="9%" height="9%">
                * 24 + 12 = 36 bits, PAE support for IA32 increased the address space to 36 bits, which supports up to 64GB of physical memory 
                * OS support is required to use PAE 
                    ** linux and Intel MAC OS X support PAE
                    ** windows 32 bit version won't support more than 4GB even though PAE is enabled 
        - x86-64 
            . intel developed IA-64(itanium) a 64 bit architecture but not widely adopted 
            . meanwhile AMD began developing a 64 bit architecture(x86-64) based on extending the existing IA-32 instruction set
            . AMD often adapting Intel architecture but in case of 64 bit Intel adopted AMD's x86-64 architecture
            . general term is x86-64 and the commercial name is AMD64 and Intel64
            . 2^64 architecture is addressable to more than 16 quintillion(16 exabytes) memory locations 
            . far fewer than 64 bits are used for address representation in current design
            . x86-64 architecture currently provides a 48 bit virtual address with support for page sizes of 4KB, 2MB or 1GB using 4 levels of paging hierarchy 
            . this addressing scheme uses PAE and can support 52-bit physical addresses (4096 terabytes)
            <img src="images/8.22.x86-64linearaddress.jpg" alt="" width="9%" height="9%">
# example: ARM architecture
    = 32 bit ARM architecture supports following page sizes 
        - 4 KB or 16 KB pages 
        - 1 MB or 16 MB pages(sections)
    = one level paging is used for 1 MB or !6 MB sections 
    = 2 level paging is used for 4 KB or 16 KB pages 
    <img src="images/8.23.logicaladdresstransilationinARM.jpg" alt="" width="9%" height="9%">
    = ARM architecture supports 2 level TLBs 
        = outer level(micro TLB). there are 2 micro TLBs  
            - data TLB 
            - instruction TLB 
        = inner(main) TLB, its a single TLB  
# extra
    = .com, is similar to .exe extension. com files are commonly executing a set of instructions whereas exe files are used for fully developed programs  
    = stub, is a small piece of code that indicates how to locate the appropriate memory resident library routine to load the library if the routine is not already present
      this stub will load only once in memory 
    = difference between RAM and associative memory(CAM,content addressable memory)
        - RAM, user supplies a memory address and RAM returns the data word stored at the address 
        - CAM, user supplies data word and CAM searches its entire memory and returns the list of addresses where the data word was located
        <img src="images/8.e.1.RAMvsCAM.jpg" alt="" width="9%" height="9%">
    = ASID(address space identifiers) a unique identifier for process and assigned by the OS
    = how many bits required to address     
        - for 4KB addresses 12bits required(2^2 is 4 and 2^10 is KB so 2+10=12) 
        - for 256MB addresses 28bits required(2^8 is 256 and 2^20 is MB so 8+20=28)
        - for 4GB addresses 32bits required(2^2 is 4 and 2^30 is GB so 2+30=32) 
    = intel chips 
        - based on segmented architecture
            . 16 bit, intel 8086, 1970 
            . 16 bit, intel 8088, later, used in original IBM PC 
        - based on paging and segmented architecture 
            . 32 bit, pentium processors, IA-32 architecture
            . 64 bit, x86-64 architecture 
    = intel designs and manufacture chips but ARM only designs then it licenses its design to chip manufactures
    = Apple has licensed the ARM design for its iPhone and iPad mobile devices
    = several Android based smartphones use ARM processor  
<!-- </details> -->
<hr>
<b id="id_virtualmemory_detail">virtual memory</b>
# background
    = virtual memory is a memory management technique where secondary memory can be used as if it were a part of the main memory
    = programmers are already limited by multiple constraints. one of them is the size of the program. if we can find a solution, the programmer fraternity will be happy
    = the instruction being executed must be in physical memory 
    = 2 approaches are      
    = place the entire logical address space in physical memory this limits the size of a program to the size of physical memory 
    = place partial program in memory and other part in secondary memory, programmer can write programs for an extremely large virtual address space , this technique is called virtual memory
    = this is made possible by swapping pages in/out of memory to secondary storage  
    = few advantages are  
        - allows the execution of processes that are not completely in memory 
        - programmer can be write programs which is larger than physical memory 
        - it allows files and memory to be shared by 2 or more processes through page sharing 
    = general layout of virtual memory is 
    <img src="images/9.1.virtualmemorylargerthanphysicalmemory.jpg" alt="" width="9%" height="9%">
    = address which programmers use are called virtual address
    = a set of virtual addresses is called virtual address space, it is a logical view of how a process is stored in memory 
    <img src="images/9.2.virtualaddressspace.jpg" alt="" width="9%" height="9%">
    = typically the logical view of the process exists in contiguous memory location and equal sized pages 
    = head grow upwards as it is used for dynamic memory allocation and stack grow downward in memory through successive function calls 
    = large blank space(hole) between the head and the stack is part of the virtual address space. this holes are known as 'sparse address space'   
    = space for spare is required in physical address space only if heap or stack grows in size 
    = using spare address space is beneficial because the hole can be filled if heap or stack grows or link the libraries dynamically during program execution 
# demand paging
    = pages are loaded only when they are demanded during program execution, this technique is called demand paging 
    = suppose a program starts with a list of available options from which the user is to select, instead of loading all 'options' code, load only the selected 'options' code to execute  
    = demand paging is a technique to implement virtual memory
    = demand paging is similar to a paging system with swapping where processes reside in secondary memory 
    = process can start quickly by demand page the page that contains the first instruction
    = only the necessary page will be swapped-in, the term is 'pager'
    <img src="images/9.3.transferofpagedspacetocontiguousdiskspace.jpg" alt="" width="9%" height="9%">
    = basic concepts 
        - before a process swapped in, the pager guess which pages will be used before this process is swapped out again and the pager will bring only those pages to memory 
        - valid-invalid bit scheme provides hardware support to identify whether page is in memory or not
            . valid, page is legal and in memory 
            . invalid, 
                * not in the logical address space of the process
                * valid(it's in the logical address space of the process) but currently in disk 
        - page table entry for a page 
            . that is brought into memory is set as usual(page table entry is frame number and valid-invalid bit) 
            . that is not currently in memory is 
                * either simply marked invalid or 
                * contains the address of the page on disk
            <img src="images/9.4.pagetablewithsomepagenotinmainmemory.jpg" alt="" width="9%" height="9%">
        - if the required pages 
            . are memory resident, then execution will proceed normally  
            . are not available in memory(marked invalid) causes a page fault. the paging hardware generate a trap to OS. trap is a result of the OS's failure to bring the desired page into memory.
        - procedure for handling page fault 
            <img src="images/9.5.stepsinhandlingapagefault.jpg" alt="" width="9%" height="9%">
            . first check whether the address requested is valid memory access or not (in an internal table kept in PCB) for this process 
            . if invalid, terminate the process 
            . if address is valid but the page is not available in memory then need to start the step for swapping in 
            . find a free frame(by taking one from the free frame list) in memory 
            . schedule a disk operation to read the desired page into the newly allocated frame 
            . when the disk read is completed, modify the internal table and page table to indicate that the page is now in memory
            . restart the instruction that was interrupted by the trap since the page is available in memory 
        - pure demand paging, never bring a page into memory until it is required 
        - program could access multiple pages for an instruction(one page for instruction and many for data), causing multiple page faults per instruction and results in unacceptable system performance
        - fortunately, analysis shows that this situation is exceedingly unlikely and programs tend to have 'locality of reference'
        - hardware to support demand paging is same for paging and swapping 
            . page table,  
            . secondary memory, the section of the disk used for the swapping purpose is called 'swap space'
        - if the page fault occurs 
            . on the instruction fetch, restart by fetching the instruction again 
            . on the operand fetch, restart by fetching and decoding the instruction again and fetch the operand 
        - some architecture provides instruction to move a lot of bytes(256) from one location to another
        - IBM System 360/370, move instruction can move 256 bytes from one location to another
        - if both blocks are in different pages and if page fault occur during the data movement then restarting will not guarantee correctness 
        - one possible scenario is data might have moved from source location but not reached destination
        - 2 possible solutions are 
            . microcode computes and attempts to access both ends of both block, page fault can occur at this early stage 
            . use temporary registers to hold the values of overwritten locations and if page fault occurs old value will be written back into memory before the trap occurs
        - so there are some architectural problems will occur if we add paging to an existing architecture especially on demand paging environment
    = performance of demand paging 
        - demand paging can significantly affect the performance of a computer system 
        - effective access time is directly proportional to the page fault rate 
        - it is important to keep the page fault rate low in a demand paging system, otherwise it will slow process execution dramatically  
        - additional aspect of demand paging is the handling and overall use of swap space 
        - swap space is faster to access than the regular file system because it doesn't have to go through the whole directory structure
        - because swap space is fast, some systems will move an entire process from the file system to swap space before the process starts, so that all future paging occurs from fast swap space
        - some system use demand paging directly from the file system for binary code(no modification expected) and reserve swap space for data segments(modification expected). eg, solaris and BSD unix
        - mobile OS typically do not support swapping instead demand paging from the file system         
# copy-on-write
    = fork() system call creates a child process that is duplicating parent's pages in the physical memory 
    = child process invoke exec() system call immediately after fork(), which will replace child pages, so duplication of pages will be an unnecessary step 
    = COW(copy on write) will overcome this unnecessary step. this will allow the parent and child to share the pages initially 
    = if either process modifies a shared page a copy of the shared page is created
    = only those pages that can be modified need to be marked as 'copy-on-write', all the unmodified pages can be shared by the parent and child processes 
    <img src="images/9.6.copyonwrite.jpg" alt="" width="9%" height="9%">
    = COW is a common technique used by several OS including windows XP,Linux and Solaris 
    = most OS will have a poll of free pages, these pages will erase previous contents before being allocated, this technique is known as 'zero-fill-on-demand' 
    = allocate zero-fill-on-demand pages when duplicating a page with COW
# page replacement
    = paging means divide process into equal sized pages and main memory to equal sized frames and keep all the pages in frames
    = virtual memory means keep all the frames in disk instead of main memory and keep only the needed frames in main memory
    = when a page fault occurs and there are no free frames (all memory is in use) in free-frame list, so OS have to decide for a possible solution
        - swap out a page frame (page replacement), it is a common solution
        - swap out a process and freeing all its frames
    <img src="images/9.7.freeframelistbeforeandafterallocation.jpg" alt="" width="9%" height="9%">
    = basic page replacement 
        - if no frame is free, find one that is currently not being used and free it
        - freeing it by writing its contents to swap space and changing the page table and all other tables to indicate that the page is no longer in memory 
        - now use the freed frame to hold the page for which the process faulted 
        - find the location of the desired page on the disk 
        - find the free frame 
            . if there is a free frame use it
            . if there is no free frame, use a page replacement algorithm to select a 'victim frame' 
            . write the victim frame to the disk, change the page and frame tables accordingly 
            <img src="images/9.8.pagereplacement.jpg" alt="" width="9%" height="9%">
        - read the desired page into the newly freed frame, change the page and frame tables 
        - continue the user process from where the page fault occurred
        - If no frames are free, 2 page transfer is required (swap in and out), which increases the effective access time
        - reduce this overhead by using a modify bit(dirty bit) entry with page table
            . modify bit will set by the hardware where ever there is a modification in page or frame 
            . if the dirty bit is modified, copy the contents of the page to disk, otherwise not needed
        - 2 algorithms needs to develop for implementing demand paging 
            . frame allocation algorithm, since multiple processes are in memory we need to decide how many frames to allocate to each process  
            . page replacement algorithm, select and replace the frames, probably the best algorithm will be the one with the lowest page fault rate 
        - evaluate an algorithm
            . run the algorithm on a string of memory reference(reference string) with the available frames then compute the number of page faults
            . reference string can be generated in 2 ways
                * artificially(eg, using a random number generator)
                * trace a process and record the address of each memory reference 
                    ** consider only the page number from the entire address 
                    ** take a page number if the page number reference is consecutive because the same page reference does not cause a page fault
            . example for evaluating an algorithm
                * by tracing a process recorded address sequence, 0100, 0432, 0101, 0612, 0102, 0103, 0104, 0101, 0611, 0102, 0103, 0104, 0101, 0610, 0102, 0103, 0104, 0101, 0609, 0102, 0105(21 addresses)
                * sequence will be reduced to 1,4,1,6,1,6,1,6,1,6,1(11 pages)
                * count of page faults 
                    ** 3 faults if 3 frames are available in memory 
                    ** 11 faults if only 1 frame, every reference results in a replacement 
            . number of frames, common perception is that by increasing the number of frames(adding physical memory) reduces the number of page faults but it is not always true 
            . as per research, for some page replacement algorithms the page fault rate may increase as the number of allocated frames increases. this result is known as belady's anomaly                    
    = Analyzing various algorithms
        - reference string : 7, 0, 1, 2, 0, 3, 0, 4, 2, 3, 0, 3, 2, 1, 2, 0, 1, 7, 0, 1 (20 page numbers)
        - number of frames : 3
        - following algorithms to analyze 
            . FIFO(first in first out) page replacement
            . OPT(optimal page replacement)
            . LRU(least recently used) page replacement
            . LRU approximation page replacement 
            . counting based page replacement 
            . page buffering algorithms 
            . applications and page replacement
    = FIFO(first in first out) page replacement 
        - simplest page replacement algorithm is FIFO
        - criteria  is "replace the oldest page"
        - FIFO uses the time when a page was brought to memory 
        - create a FIFO queue to hold all pages in memory
        <img src="images/9.9.FIFOpagereplacement.jpg" alt="" width="19%" height="9%">
        - Belady's anomaly can occur in FIFO algorithm. consider the following reference string 1,2,5,1,2,3,4,5 with 3 and 4 frames
        <img src="images/9.10.beladysanomalyFIFOpagereplacement.jpg" alt="" width="29%" height="9%">
    = OPT(optimal page replacement) 
        - this algorithm has the lowest page fault rate for a fixed number of frames of all algorithms also called OPT or MIN
        - criteria is "replace the page that will not be used for the longest period of time" 
        - OPT is uses the time when a page is to be used in the future
        - never suffer from belady's anomaly 
        <img src="images/9.11.optimalpagereplacement.jpg" alt="" width="19%" height="9%">
        - a reference to page 2 replaces page 7 because page 7 is not used until reference 18
        - unfortunately this solution is hard to implement and it is used mainly for comparison studies 
    = LRU(least recently used) page replacement 
        - since OPT is not feasible to implement perhaps an approximation of OPT is possible 
        - criteria is "replace the page that has not been used for the longest period of time"
        - LRU uses the recent past as an approximation of the near future
        - never suffer from belady's anomaly
        <img src="images/9.12.leastrecentlyusedpagereplacement.jpg" alt="" width="19%" height="9%">
        - a reference to page 4 replaces page 2 because page 2 was least recently used
        - LRU is considered to be better than FIFO (12 vs 15 faults)
        - the problem is determining the order of frames given the last usage time, implementation for this solution require substantial hardware assistance
        - 2 possible implementations are 
            . counters 
                * associate each page table entry with a time-of-use field and add a logical clock or counter with the CPU 
                * clock is incremented for every memory reference and the content of this register(time of the reference) are copied to the time-of-use field
                * page with smallest time value will be replaced 
            . stack  
                * keep a stack of page numbers 
                * whenever a page is referenced it is removed from the stack and put on the top. most recently used page is always on top and least recently used always on bottom  
                * best to implement using doubly linked list with a head and tail pointers. there is no search for a replacement 
                * tail pointer points to the bottom of the stack which is the LRU page 
        - both OPT and LRU is belong to a class of page replacement algorithm called stack algorithm
        - stack algorithm, the pages kept in memory for a frame set of n are always a subset of frame size n+1
        - both OPT and LRU won't exhibit belady's anomaly because both are stack algorithm class
    = LRU approximation page replacement 
        - some systems provide sufficient hardware support , some are not and some provides some help in the form of 'reference bit'
        - a reference bit is associated with each entry in the page table and is set by the hardware whenever that page is referenced (reading or writing any byte in the page)
        - all bits are initially set to '0' by the OS and set to '1' when a page is referenced
        - although the order of usage is not known but we know which pages were used and which were not. this information is the basis for many page replacement algorithms that approximate LRU replacement 
        - additional reference bits algorithm
            . this algorithm including additional ordering information
            . the latest 8 reference information for each page is stored in a separate table in one byte of memory(orderRef)
            . at regular interval a timer interrupt transfers control to the OS
            . OS will shift the 'reference bit' to the high-order(left) bit of the 'orderRef', right-shift the existing bits of the 'orderRef' bit by one, and discard the low-order bit
            <img src="images/9.13.additionalreferencebitsalgorithm.jpg" alt="" width="19%" height="9%">
            . if the orderRef contains 00000000 then the page has not been used for 8 time periods and a page used at lease once in every interval 11111111
            . a page with lowest number in LRU page can be replaced
            . select all pages with the smallest value or use FIFO to replace
            . the number of bits of history included in the shift register can be varied depending on the hardware availability  
        - second chance algorithm
            . this algorithm is a FIFO replacement algorithm also called clock algorithm
            . when a page is selected and inspect the reference bit 
                * if bit is 0 then proceed to replace the page 
                * if bit is 1 then following 3 things will happen and move on to select next FIFO page
                    ** give the page a second chance                    
                    ** reset reference bit to 0(cleared)
                    ** reset arrival time to current time 
            . a page that is given a second chance will not be replaced until all other pages replaced or given a second chance 
            . in addition if a page is used often enough will never be replaced 
            . one way to implement a second chance algorithm as a circular queue 
            <img src="images/9.14.secondchance(clock)pagereplacementalgorithm.jpg" alt="" width="9%" height="9%">
            . a pointer indicates which page is to be replaced next 
            . when a frame is needed the pointer advances until it finds a page with a 0 reference bit
            . once a victim page is found the page is replaced and the new page is inserted in the queue in that position  
            . in worst case if all bits are set then 'second chance algorithm' degrades to FIFO  
        - enhanced second chance algorithm 
            . enhancing the second chance algorithm by considering the reference bit and the modify(dirty) bit as an ordered pair. 4 possible classes 
                * (0,0) recently not used and nor modified, best page to replace 
                * (0,1) recently not used and modified, need to write before replacing 
                * (1,0) recently used and not modified, second chance 
                * (1,1) recently used and modified, second change and need to write before replacing
            . this algorithm searches the page table in a circular fashion(as many as 4 passes), first pass is looking for a first page with (0,0) if not then second pass for a page with (0,1) etc 
    = counting based page replacement
        - there are several algorithms based on counting the number of references to the given page 
        - LFU(least frequently used), replace the page with the lowest reference count 
        - MFU(most frequently used), replace the page with the highest reference count 
        - in general count based algorithm is not commonly used as the implementation is expensive and do not approximate OPT well  
    = page buffering algorithms 
        - system keep a pool of free frames 
        - when a page fault occurs a victim frame is chosen and the desired page is read into a free frame on the pool before the victim is written out 
        - this procedure allows the process to restart as soon as possible without waiting for the victim page to be written out  
        - later when the victim page is written, its frame is added to the free frame pool
        - expansion of this idea is to maintain a list of modified pages and whenever the paging device is idle the modified page is selected and is written to the disk
        - another modification is to keep a pool of free frames, but remember which page is in each frame, because it can be reused if the page is replaced
    = applications and page replacement 
        - some applications (especially DB programs) should be assigned to perform their own memory management because they understand data access and caching requirements better than a general-purpose OS
        - such programs are given a raw disk partition(raw data blocks and no file system structure) to work on. It is up to the application to use this as extended memory
# allocation of frames
    = 2 important task in memory management are page replacement strategy and a frame allocation strategy
    = if we have 93 free frames and 2 processes how many frames will each process get?
    = simple case 
        - single user system with 128KB of memory composed of pages 1KB per size, so this system have 128 frames 
        - OS may take 35KB leaving 93 frames for the user process 
        - in pure demand paging, all 93 frames would initially be put on the free frame list 
        - when user process starts execution it would generate a sequence of page faults and first 93 page faults would all get free frames from the free frame list 
        - when all the free frames are exhausted, a page replacement algorithm would be used to select one on the in memory 93 frames for replacement 
        - when the process terminated, all these frames would be placed on the free frame list 
    = minimum number of frames 
        - allocate minimum number of frames is dependent on system architecture, and corresponds to the worst case scenario of the number of pages that could be touched by a single instruction
        - if an instruction spans a page boundary then multiple pages could be needed just for the instruction fetch 
    = allocation(equal allocation) algorithm
        - allocate equal number of frames 
        - m is available frames and n is processes frames so m/n frames for each processes 
        - m=93 and n=5 so 93/5=18 and 3 reminder
        - 18 frames per process and 3 frames will be added to the free frame list
    = proportional allocation
        - allocate proportional number of frames  
        - m=93 and n=5
        - frame requirement for each process p1=10, p2=30, p3=30, p4=16, p5=7 
        - S = sum(pi)
        - (pi/S) * m 
        - (10/(10+30+30+16+7)*90) = 10 frames for p1
        - in equal and proportional allocations high and low priority process are treated equally 
    = global vs local allocation 
        - classification of5445 page replacement algorithms into 2 broad categories 
            . global replacement 
            . local replacement 
        - global replacement allows to select from any process even if the frame is currently allocated to some other process ie, one process can take a frame from another 
        - local replacement requires that each process select from only its own set of allocated frames 
        - global page replacement is overall more efficient and is the more commonly used approach  
    = non uniform memory access 
        - if the CPU and memory are on the same board, the memory can be accessed with low latency, but if they are on different boards, the memory access latency increases
        - systems in which memory access times vary significantly are known collectively as NUMA(non-uniform memory access)
    = extra 
        - lgroups(latency group), each lgroup gathers together close CPUs and memory together in the kernel
# thrashing
    = if the frame allocation for a process falls below the minimum frames required 
        - OS should suspend execution of that process
        - swap out its remaining pages and freeing all its allocated frames(intermediate CPU scheduling)  
    = if a process doesn't have enough frames, it can quickly cause a page fault and replace the page. if the replaced page is needed immediately it will cause a page fault again
    = repeating the above procedure over and over again is called thrashing
    = a process is called trashing if it is spending more time paging than executing 
    = thrashing results in severe performance problems 
    = early process scheduling schemes would control the DOM(degree of multiprogramming) based on CPU utilization. if it is low, new process will be introduced to increase the DOM
    = thrashing due to global page replacement
        - a global page replacement algorithm is used to replace pages regardless of the process which they belong to
        - if a process needs more frames, it will starts faulting and starts taking frames from other processes 
        - these faulting processes use page device to swap pages in and out
        - the ready queue becomes empty when they queue up for page devices and the CPU utilization decreases 
        - the CPU scheduler sees the decreasing CPU utilization and increases the DOM as a result 
        - the new process tries to get started by taking frames from the running processes this causes more page faults and longer queue for the paging devices 
        - again CPU utilization further drops and the CPU scheduler tries to increase DOM even more 
        - no work is getting done because processes are spending all their time paging 
    = thrashing due to local page replacement
        - a local(priority) page replacement algorithm replaces only its own process's pages, so thrashing of other process will not affect the process 
        - but this replacement cannot provide a complete solution because trashing process will be queued for the page device, so the average service time per page fault will increase
        - eventually, even a non-thrashing process will have an increased effective access time
    <img src="images/9.15.thrashing.jpg" alt="" width="9%" height="9%">
    = thrashing will occur at some point as we increase the DOM even if uses local or global replacement algorithm 
    = to prevent thrashing we must provide processes with as many frames as they really need right now
    = techniques to identify the suitable number of frames are 
        - working set model 
        - page fault frequency model
    = working set model 
        - working set model is based on the concept of locality and defines a working set window of length delta(Δ)
            . as a process executes it moves from locality to locality 
            . a locality is a set of pages that are actively used together 
            . a program is generally composed of several different localities 
            . localities are defined by the program structure and its data structure 
        - the idea is to examine the most recent page references (delta)
        - working set is a moving window. on each memory reference a new reference appears at one end and the oldest reference drops off the other end 
        - the set of most recent page reference is called 'working set'
            . if a page is in active use, it will be in the working set
            . if a page is no longer in use, it will drop from the working set 
        - accuracy of working set depends on the selection of delta. if delta is 
            . small, it will not encompass the entire locality 
            . large, it will overlap several localities 
            . infinite, it will touch all the pages touched during the process execution  
        - example
            <img src="images/9.16.workingsetmodel.jpg" alt="" width="19%" height="9%">
            . delta(Δ) = 10 
            . in time 1(t1) for last 10 references the working set is {1,2,5,6,7} from the set of referenced pages {1,5,7,7,7,7,5,2,6,2}
            . in t2 for last 10 references the working set is {3,4} from the set of referenced pages {4,4,4,3,4,3,4,4,4,3} 
        - most important property of the working set is its size 
        - first need to compute the total demand of the pages(D) which is D = ΣWSSi
        - each process is actively using the pages in its working set so for process i needs WSSi frames 
        - if available frame is less than the demand (D>m) thrashing will occur 
        - once Δ is selected, use of the working set is simple 
        - OS monitors the working set of each process and allocates enough frames to work with 
        - if there are enough extra frames, another process can be initiated 
        - if the total number of working sets increases and the total number of available frames decreases, the OS will suspend the process, swap all pages, and transfer the frames to other processes
        - working set model prevents thrashing while keeping the degree of multiprogramming as high as possible but keep track of the working set is difficult since its a moving window 
        - if we do not allocate enough frames to accommodate the size of the current locality then the process will thrash (total size of locality > total memory size)
    = page fault frequency(PDF)
        - a more direct approach to controlling thrashing is PDF
        - establish upper and lower bounds based on the page fault rate 
        - if page fault rate 
            . exceeds the upper limit, allocate more frames to the process by suspending the process(by OS), swap all pages, and transfer the frames to other processes 
            . falls below the lower limit, remove frames from the process 
        <img src="images/9.17.pagefaultfrequency(PDF).jpg" alt="" width="9%" height="9%">
    = trashing and the resulted swapping will have a large impact on performance 
    = current best practice in implementing a computer facility is to include enough physical memory to avoid trashing and swapping from smartphones through mainframes 
# memory-mapped files
    = a sequential file access requires standard system call(open,read and write) and disk access
    = alternatively, file IO can be treated as a normal memory access using a virtual memory technique. this approach known as memory mapping a file 
    = memory mapping allows a portion of the virtual address space to be logically associated with a file. this can result significant performance increases 
    = basic mechanism 
        - memory mapping a file is accomplished by mapping a disk block to a page(or pages) in memory
        - initial access to the file proceeds through ordinary demand paging(page fault) 
        - a page sized portion of the file is read from the file system into a physical page(some system may opt to read more than a page-sized chunk of memory at a time)
        - subsequent reads and writes to the file are handled as routine memory accesses 
        - writing to disk for a memory-mapped file may not be immediate (synchronous)
            . during a periodic check, updates to disk as part of a periodic check to see if the file has been modified in memory 
            . when the file is closed, all memory-mapped data is written to disk and removed from virtual memory when the file is closed
        - different systems calls
            . open(), read(), write() for standard system call
            . mmap() for memory mapped specific system call
        - solaris treats all file IO as memory mapped 
            . memory mapped specific system call, map the file into the address space of the process 
            . standard system call, map the file into the address space of the kernel 
        - multiple processes share same file concurrently by memory mapping a file 
        <img src="images/9.18.memorymappedsharingfiles.jpg" alt="" width="9%" height="9%">  
    = shared memory in the windows API 
        - windows implements shared memory using shared memory mapped files. 3 steps are 
            . create a file, producing a HANDLE to a new file 
            . make it a named shared object, producing a HANDLE to the shared object 
            . map it to virtual memory address space, returning its base address as a void pointer(LPVOID)
        <img src="images/9.19.sharedmemoryusingmemorymappedIO.jpg" alt="" width="9%" height="9%">
    = memory mapped IO 
        - in the case of IO, each IO controller includes registers to hold commands and data being transferred. special IO instructions allow data transfers between registers and memory 
        - memory mapped IO is implemented in many computer architecture to have more convenient access to the IO devices 
        - for some devices, it makes sense to map the device controller's registers to addresses in the process's virtual address space. eg, video controller cards
        <img src="images/9.20.devicecontrollermemorymappedIO.jpg" alt="" width="9%" height="9%">
        - serial and parallel devices can also use memory mapped IO, mapping the device registers(IO port) to memory addresses
        - CPU writes a data byte to the data register and sets a bit in the control register. device take the data and clears the control bit to signal that the device is ready for the next byte
        - CPU implements 2 different methods to identify the status of the control register
           . programmed IO, CPU constantly pool/loop the control register to watch the control bit
           . interrupt driven IO, CPU receives an interrupt when the device is ready for the next byte
# allocating kernel memory
    = when a process running in user mode and request for additional memory, pages are allocated from the list of free page frames maintained by the kernel 
    = kernel memory is often allocated from a free memory pool different from the list used to satisfy ordinary user mode processes
    = 2 primary reasons on having a different free memory pool
        - many OS do not subject kernel code or data to the paging system, and the allocated space may be less than a page size 
        - unlike user mode, kernel mode requires memory to reside in physically contiguous pages
    = 2 strategies for managing free memory assigned to kernel processes 
        - buddy system 
            . buddy system allocates memory from a fixed size segment consisting of physically contiguous pages
            . memory is allocated from this segment using a 'power-of-2 allocator', which satisfies requests in units sized as a power of 2(4KB,8KB,16KB and so forth)
            . a request will satisfy with the big enough size, for 11KB round to the next highest power 16KB segment 
            . example 
                * size of memory segment is initially 256KB and the request is for 21KB
                * total segment is divided into 2 buddies, AL and AR each of 128KB and one of the buddy can be further divided into 2 buddies of 64KB(BL,BR) each then further to 32KB(CL,CR) each
                * next highest power of 21KB is 32KB 
                <img src="images/9.21.buddysystemallocation.jpg" alt="" width="9%" height="9%">
            . advantage 
                * buddies can be combined to form larger segments using a technique known as 'coalescing'(combine). ultimately end up with the original 256KB segment 
            .disadvantage
                * rounding off to the next highest power is very likely to cause fragmentation within allocated segments 
        - slab allocation  
            . the basic idea behind the slab allocator is to have caches of commonly used objects kept in an initialized state available for allocation in the kernel
            . a 'slab' is made up of one or more physically contiguous pages 
            . a 'cache' is made up of one or more slabs
            . cache represents a small amount of very fast memory. a cache is a storage for a specific type of object such as cache representing semaphore stores instances of semaphore objects 
            . each cache is populated with objects that are instantiations of the kernel data structure like instances of semaphore objects 
            <img src="images/9.22.slaballocation.jpg" alt="" width="9%" height="9%">
            . slab allocation algorithm uses caches to store kernel objects and initially all objects marked as 'free' 
            . number of objects in cache depends on the size of the associated slab. 12KB slabs (3*4KB pages) should store 6(6*2KB) objects
            . when a new object for a kernel data structure is needed the allocator can assign any free object from the cache and it is marked as 'used'
            . the cache will fulfill the request using respective object that has already been allocated in a slab and is marked as free 
            . in linux, a slab may be in 3 possible states
                * Full, all objects in the slab are 'used' 
                * Empty, all objects in the slab are 'free' 
                * Partial, both 'used' and 'free' objects 
            . order of slab allocation with a free object
                * first, try to satisfy from partial slab
                * next from an empty slab 
                * last, new slab is allocated from contiguous physical pages and assigned to a cache. memory for the object is allocated from this slab
            . advantage
                * no internal fragmentation, each kernel data structure is associated with a cache, and the cache is divided equally by the size of the objects and correct amount of memory can be allocated
                * since memory for the objects are created in advance thus can be quickly allocate from cache. in case of release the object will be marked as free and returned to its cache 
    = linux originally used baddy system however from version 2.2 linux adopted the slab allocator(linux refers the implementation as SLAB)
    = recent distribution include 2 other kernel memory allocators 
        - SLOB 
        - SLUB(from 2.6.24 onwards default)
# other considerations
    = major decisions to make for a paging system are the selection of
        - a replacement algorithm
        - an allocation policy 
    = other points of consideration
        - prepaging 
        - page size 
        - TLB reach 
        - inverted page tables 
        - program structure 
        - IO interlock and page locking
    = prepaging     
        - page faults will increase in 2 occasions
            . when a process starts 
            . when a swapped out process is restarted 
        - prepaging is an attempt to prevent this increase in initial paging  
        - strategy is to bring all the required pages into memory at one time
        - solaris, prepage the page frames for small files 
        - systems with working-set model, will bring all the pages mentioned in the working set to the memory when the process is restarting 
        - implementing prepaging is beneficial if the cost is less than the cost of page faults
    = page size 
        - a page size must be decided when designing a new machine and there is no single best page size 
        - page size is invariably powers of 2, generally ranging from 4096(2<sup>12</sup> or 4KB) to 4,194,304(2<sup>22</sup> or 4MB) bytes
        - page table a concern while deciding on the page size. decreasing the page size increases the number of pages and hence the size of the page table 
        - each active process should have its own copy of the page table, and a large page size is desirable
        - for a virtual memory of 4MB, there can be 4096 pages of 1KB each or 512 pages of 8KB each 
        - decision on smaller pages va large pages
            . smaller pages, memory is better used with smaller pages to minimize internal fragmentation
            . larger pages, large page to minimize IO time
                * at a transfer rate of 2MB/sec
                * transferring a page size of 512 bytes will take 28.2 milliseconds(transfer of 0.2, latency of 8 mils and seel of 20 mils)
                * transferring a page size of 1024 bytes will take 28.4 milliseconds(transfer of 0.4, latency of 8 mils and seel of 20 mils)
                * for transferring of two 512 bytes pages will take 56.4 mils but for a single page of 1024 byte will take 28.4 mils so larger page size is desirable
            . smaller pages, less IO and less total allocated memory
                * if a page size is 100KB and the process just need 25KB for execution then the IO and memory allocation for 75KB is unwanted 
            . larger pages, large page for minimizing the number of page faults and overhead 
                * every page fault will have the overhead of processing the interrupt, saving register, replacing a page, queueing for the page device and updating table
        - historically trend is toward larger page sizes even for mobile systems
        - page size on my current machine(Dec 2022) is 4096 bytes(4KB)
    = TLB reach 
        - TLB reach is a matrix similar to hit ratio
        - TLB reach refers to the amount of memory accessible from the TLB, number of TLB entries * page size 
        - ideally working set for a process is stored in TLB 
        - approaches for increasing TLB reach
            . double the number of entries in the TLB to double the TLB reach 
            . increase the size of the page 
            . provide pages of different page sizes
        - ultraSPARC supports page sizes of 8KB,64KB,512KB and 4MB and it uses both 8KB and 4MB pages
        - managing of different page size needs support from OS and it comes at a cost in performance
        - recent trends indicate a move towards SW-managed TLBs and OS support for multiple page sizes 
    = inverted page tables 
        - inverted page table contains the mapping information about page to frame 
        - inverted page table no longer contains complete information about the logical address space of a process and these details will be available in the traditional per process page table 
        - since the details are missing and require page fault and demand paging to get the details 
    = program structure 
        - careful selection of data structures and programming structures can increase locality and hence lower the page fault rate and the number of pages in the working set 
    = IO interlock and page locking
        - when demand paging is used some pages needs to be locked in memory. 
        - lock bit is associated with every frame and it cannot be selected for replacement
# operating-system examples
# extra
    = swap space is space used to make extra space for processes, usually on disk, even when main memory is full
    = memory mapping, is a mechanism that maps a file on disk to a range of addresses within an application's address space. then the application can access files on disk in the same way it access memory 
    = anonymous memory, is a memory mapping with no file or device backing 
    = pure demand paging, never bring a page into memory until it is required
    = a sequential file that contains and stores data in sequential order. it must read from scratch into the desired data location and store it in sequential access devices such as magnetic devices
    = random access, a file can be accessed non-sequentially or randomly or the contents of files can be accessed directly like seek a location directly to read or write. these files act like an array 
    = serial transmission of data, bit by bit gets transferred 
    = parallel transmission of data, a byte or a character gets transferred 
    = IO port is a socket on a computer that a cable can plugged into. this port connects the peripheral device to the CPU 
    <img src="images/9.e.1.IOports.jpg" alt="" width="9%" height="9%">
    = process descriptors, a set of information related to a single process for instance a process priority, whether it is running on a CPU or blocked on an event. 'task_struct' is the type structure
    = seek, it is the time it takes the head to move from the current track to where the data is
    = latency, it is the time to move from the current sector to the sector where data resides
    = transfer, it is the time from the start of the transfer to the completion of the transfer  
    = 1000 = KB, 1,000,000 = MB, 1,000,000,000 = GB
</details>
<hr>
<b id="id_massstoragestructure_detail">mass-storage structure</b>
# overview of mass-storage structure
    = since the main memory is too small to hold all the information (data and programs) permanently, the computer system must provide secondary storage to back up the main memory
    = magnetic disk
        - magnetic disk provide the bulk of secondary storage for modern computer systems 
        - each disk platter has a flat circular shape like a CD. common platter diameters range from 1.8 to 3.5 inches. both sides of the platter are covered with a magnetic material
        - magnetic disk store information by recording it magnetically on the platters
        <img src="images/10.1.harddrive.jpg" alt="" width="9%" height="9%">
        - bit density reached upto 20 Tbits/in<sup>2</sup> and more
        - a read-write head attached to a disk arm moves from one side of the platter to the other
        - platter is logically divided in to circular tracks and track subdivided into sectors
        - a set of tracks on all surfaces of the same diameter called a cylinder
        - disk drive spindle motor spins at a high speed cause the disk to rotate 
        - data speed has 2 parts (transfer rate + positioning rate)
            . transfer rate, the rate at which data flow between the drive and the computer
            . positioning rate, it has 2 parts 
                * seek time, time to move the disk arm to the desired cylinder 
                * rotational latency, necessary time for the desired sector to rotate to the disk head 
        - disk drive is attached to a computer by a set of wires called an IO bus 
        - several kind of buses 
            . advanced technology attachment(ATA)
            . serial ATA(SATA)
            . eSATA
            . universal serial bus(USB)
            . fibre channel(FC)
        - data transfer on a bus are carried out by special electronic processors called controllers 
            . a disk controller converts the signals read by a disk's read and write head and transmits them to the peripheral bus
            . a host adapter enables computer system to communicate with a peripheral bus. converts the signals from peripheral bus to appropriate format for the motherboard's bus. finally the CPU reads it
            . host controller(adaptor), a controller at the computer end of the bus and disk controller built into each disk drive usually a circuit board covering the bottom of the drive 
            <img src="images/10.2.diskcontrollerandhostadaptor.jpg" alt="" width="9%" height="9%">
        - data transfer working 
            . a computer places a command into the host controller typically using memory mapped IO ports
            . the host controller then sends the command via messages to the disk controller
            . the disk controller operates the disk drive hardware to carry out the command
            . disk controller usually have a built in cache and the data transfer at the disk drive happens between surface and cache
            . data transfer to the host controller happens between the disk controller cache and the host controller
    = solid state disk
        - a non volatile memory that is used like a hard drive 
        - more reliable than HDD because it doesn't have moving part, no seek time and latency, consume less power 
        - more expensive per MB than HDD, less capacity than HDD, shorter life span than HDD
    = magnetic tapes 
        - tapes are used for backup for storage of infrequently used information and as a medium for transferring information from one system to another 
        - positioning or moving to correct spot for reading or writing data will may takes minutes 
        - once positioned the speed is comparable to disk drive 
# disk structure
    = modern magnetic disk drives are addressed as large one dimensional arrays of logical blocks
    = logical block is the smallest unit of transfer usually 512 bytes, some disks can be low-level formatted to have a different logical block size such as 1024 bytes
    = one dimensional array of logical blocks is mapped onto the sectors of the disk sequentially 
    = sector 0 is the first sector of the first track on the outermost cylinder
    = mapping continues in sequence through that track, then through the rest of the tracks on that cylinder, then through the rest of the cylinder from outermost to innermost 
    = there are 2 ways to store data
        - constant linear velocity(CLV), density of bits on every track is uniform. as a result the outermost tracks typically hold 40% more sectors than do tracks in the innermost zone
        - constant angular velocity(CAV), bit density is reduced from the inner tracks to the outer tracks to keep the data rate constant
    = to overcome CLV deficiency, drive increases its rotation speed as the head moves from the outer to the inner tracks to keep the same rate of data moving under the head
    = eg,drive can read x sectors/sec, 100/1 from outer track but 40/1 so drive will increase the speed to match data intake
    = as disk technology improves the number of sectors per track and the number of cylinders per disk are increasing
# disk attachment
    = computer will access the disk storage in 2 ways 
        - via IO ports(host attached storage) common in small systems 
        - via a remote host in a distributed file system(network attached storage)
    = host attached storage 
        - storage accessed through local IO ports 
        - these ports use several technologies 
            . IDE or ATA or SATA, typical desktop PC uses IO bus architecture called IDE or ATA or SATA 
            . FC(fibre channel), servers or high end workstations uses FC. FC protocol has 2 variants 
                * large switched fabric, this scheme is used to address ports in the switched fabric and each port has its own unique 24-bit address. this is the basic of storage area networks(SAN)
                * arbitrated loop(FC-AL), is a fibre channel topology in which devices are connected in a one way loop fashion in a ring topology. it was a lower-cost alternative to a fabric topology 
        - wide variety of storage devices are suitable for use as host attached storage 
            . hard disk drive
            . RAID arrays 
            . CD 
            . DVD 
            . tape drives 
    = NAS(network attached storage)
        - NAS device is a special purpose storage system that is accessed remotely over a data network 
        - clients access NAS via a remote procedure call interface such as NFS for unix or CIFS for windows 
        - RPC calls are carried via TCP or UDP over an IP network usually the same LAN that carries all data traffic to the clients  
        - NAS unit is usually implemented as a RAID array with software that implements the RPC interface
        - NAS provide a convenient way for all the computers on a LAN to share a pool of storage with but it is less efficient and low performance than direct attached options
        - iSCSI is the latest NAS protocol, it uses IP network protocol to carry the SCSI protocol, thus networks can be used as the interconnects instead of SCSI cables  
        <img src="images/10.3.networkattachedstorage.jpg" alt="" width="9%" height="9%">
    = SAN(storage area network)
        - communication between servers and clients competes for bandwidth with the communication among servers and storage devices 
        - SAN is a private network using storage protocols rather than networking protocols connecting servers and storage units 
        - power of SAN lies in its flexibility. multiple hosts and multiple storage arrays can attach to the same SAN and storage can dynamically allocated to hosts 
        - SAN interconnects are 
            . FC is the most common
            . iSCSI is increasing its use
            . infiniband, a special purpose bus architecture 
        <img src="images/10.4.storageareanetwork.jpg" alt="" width="9%" height="9%">
# disk scheduling
    = IO work flow (to or from disk) required by a process
        - process issues a system call to the OS . this request specifies following informations 
            . is it an input or output 
            . disk address for transfer 
            . memory address for transfer 
            . number of sectors to be transferred  
        - servicing the request 
            . request can be serviced immediately if the desired disk drive and controller are available 
            . request will be placed in the queue of pending requests for that drive, if the drive and controller is busy 
        - in a multiprogramming environment, the OS will decide which IO request to service next from the pending queue list
        - the OS will use some disk scheduling algorithm to decide which IO request to service next from the pending queue list
    = evaluating various algorithms
        - a disk with 200 cylinder
        - a disk queue requesting IO for the following blocks on cylinders (98,183,37,122,14,124,65,67)
        - currently the head is positioned at 53rd cylinder  
    = FCFS scheduling 
        - FCFS is first come first serve algorithm 
        - full head movement and service from 53-98-183-37-122-14-124-65-67
        - total head movement of 640 cylinders [(98−53)+(183−98)+(183−37)+(122−37)+(122−14)+(124−14)+(124−65)+(67−65)]
    = SSTF scheduling 
        - service the request close to the current head position before the head move faraway to service other requests 
        - SSTF(shortest seek time first) algorithm
        - full head movement and service from 53-65-67-37-14-98-122-124-183
        - total head movement of 236 cylinders [(65−53)+(67−65)+(67−37)+(37−14)+(98−14)+(122−98)+(124−122)+(183−124)]
        - this is similar to shortest job first(SJF) scheduling and this algorithm will suffer from starvation of some requests
        - this algorithm is a substantial improvement over the FCFS algorithm but it is not optimal
    = SCAN scheduling 
        - head movement and processing
            . moves the head from the initial position to one edge of the disk and processes requests along the way
            . immediately head to the other end and processes requests along the way   
            . the algorithm forces the head to move both extreme end cylinders even though there is no request to process   
        - some time its called elevator algorithm, since the disk arm behave like an elevator in a building
        - assuming that the disk arm is moving towards 0 from the initial head position of 53 
            . full head movement and service from 53-37-14-0-65-67-98-122-124-183
            . total head movement of 236 cylinders [(53-0) + (183−0)]
        - assuming that the disk arm is moving towards 199 from the initial head position of 53 
            . full head movement and service from 53-65-67-98-122-124-183-199-37-14
            . total head movement of 331 cylinders [(199-53) + (199−14)]
    = C-SCAN scheduling 
        - circular scan scheduling is a variant of SCAN to provide a more uniform wait time 
        - head movement and processing
            . moves the head from the initial position to one edge of the disk and processes requests along the way
            . immediately head to the other end without processing any requests along the way
            . on the subsequent return trip processes requests along the way 
            . the algorithm forces the head to move both extreme end cylinders even though there is no request to process
        - assuming that the disk arm is moving towards 0 from the initial head position of 53 
            . full head movement and service from 53-37-14-0-199-183-124-122-98-67-65
            . total head movement of 386 cylinders [(53-0) + (199-0) + (199-65)]
        - assuming that the disk arm is moving towards 199 from the initial head position of 53 
            . full head movement and service from 53-65-67-98-122-124-183-199-0-14-37
            . total head movement of 382 cylinders [(199-53) + (199−0) + (37-0)]
    = LOOK and C-LOOK scheduling 
        - both SCAN and C-SCAN move the disk arm across the full width of the disk
        - but in practice neither algorithm is often implemented this way and the arm goes only as far as the final request in each direction 
        - versions of SCAN and C-SCAN that follow this pattern are called LOOK and C-LOOK scheduling because they look for a request before continuing to move in a given direction 
        - LOOK scheduling, assuming that the disk arm is moving towards 0 from the initial head position of 53 
            . full head movement and service from 53-37-14-65-67-98-122-124-183
            . total head movement of 208 cylinders [(53-14) + (183−14)]
        - LOOK scheduling, assuming that the disk arm is moving towards 199 from the initial head position of 53 
            . full head movement and service from 53-65-67-98-122-124-183-37-14
            . total head movement of 347 cylinders [(183-53) + (183−14)]
        - C-LOOK scheduling, assuming that the disk arm is moving towards 0 from the initial head position of 53 
            . full head movement and service from 53-37-14-183-124-122-98-67-65
            . total head movement of 326 cylinders [(53-14) + (183-14) + (183-65)]
        - C-LOOK scheduling, assuming that the disk arm is moving towards 199 from the initial head position of 53 
            . full head movement and service from 53-65-67-98-122-124-183-14-37
            . total head movement of 322 cylinders [(183-53) + (183-14) + (37-14)]
    <img src="images/10.5.examplesfordiskschedulingalgorithms.jpg" alt="" width="19%" height="9%">
    = selection of a disk scheduling algorithm 
        - SSTF has a natural appeal because it increased performance over FCFS 
        - SCAN and C-SCAN perform better for systems that place a heavy load on the disk because they are less likely to cause a starvation problem 
        - disk scheduling algorithm should be written as a separate module of the OS so that it can be replaced with a different algorithm if necessary 
        - either SSTF or LOOK is a reasonable choice for the default algorithm 
# disk management
    = disk formatting 
        - a new magnetic disk is a blank slate, it is just a platter of a magnetic recording material 
        - physical formatting or low level formatting, before data can be stored in disk, it must be divided into sectors that the disk controller can read and write 
        - low level formatting fills the disk with a special data structure for each sector 
        - data structure for each sector typically consists of a header, a data area(usually 512 bytes), and a trailer 
        - header and trailer contains information used by the disk controller such as sector number and error correcting code(ECC)
        - when the controller 
            . writes a sector of data, ECC is updated with a value calculated from all the bytes in the data area 
            . reads the sector, ECC is recalculated and compared to the store value. If the value is different, it means that the data area of ​​the sector is corrupted and the data sector may be bad
        - controller automatically does the ECC processing whenever a sector is read or written 
        - disk formatting process involves 
            . low level formatting 
            . partitioning 
            . high level formatting 
        - low level formatting is a type of physical formatting 
            . it is the process of marking the cylinders and tracks of a blank hard disk, then division of tracks into sectors with the sector markers 
            . now a days low level formatting is performed by the hard disk manufactures themselves 
            . it must be told how many bytes of data space to leave between the header and trailer in each sector
            . usually choose among from the size 256, 512 and 1024 bytes. Some OS can handle only 512 bytes 
            . after low level formatting it is impossible to recover data from the disk
        - partition, the disk into one or more groups of cylinders. OS can treat each partition as it were a separate disk, ie one partition hold OS executable code and other holds user files  
        - logical formatting is the process of
            . setting up an empty file system on a disk partition or a logical volume like NTFS,FAT32, exFAT or other formats
            . installing boot sector 
        - most file systems group blocks together into larger chunks called clusters 
        - disk IO is done via blocks and file system IO is done via clusters  
        - OS gives certain programs the ability to use a disk partition as a large array of logical blocks without file system data structures called raw disks and IO to this array is called raw IO
    = boot block 
        - when a computer is powered up or rebooted it must have an initial program to run, bootstrap
        - bootstrap program do the following steps
            . initializes all aspect of the system(CPU registers, device controllers and content of main memory)
            . finds the OS kernel on disk
            . loads that kernel into memory
            . jumps to an initial address to begin the OS execution 
        - bootstrap program is conveniently stored in read-only memory (ROM) so it does not require an initialization, and is in a fixed location so the processor can execute it at power up or restart
        - but to change this program code or fix the error, the ROM hardware chips need to be changed
        - as a cost effective solution 
            . most systems store only a tiny bootstrap loader program in the boot ROM
            . a full bootstrap program stored in the boot blocks at a fixed location on the disk. this partition might contain OS and device drivers 
            . the tiny bootstrap loader program instructs the disk controller to read and load the boot blocks(full bootstrap program) into memory
            . full bootstrap program will load entire OS from a non-fixed location on disk and start the OS. device drivers are not loaded at this point
        - a disk that has a boot partition is called a boot disk or system disk 
        - in the x86 world bootstrap is often explained in 2 stages 
            . first stage bootloader(BIOS) 
            . second stage bootloader(GRUB,MBR & NTLDR)
        - booting steps in the x86 world
            . when a computer is powered up, BIOS is loaded, it will scan all the attached devices and prepare them to be in a form that OS can use 
            . after the initial work, BIOS will load the boorloader which is a part of OS and does the job of loading the OS
            <img src="images/10.6.bootingworkflow.jpg" alt="" width="9%" height="9%">
        - outside of X86 world(phone, embedded devices,etc) the term BIOS is usually not used all stages of boot loaders are often called boot loader
        - windows as an example 
            . windows allows a hard disk to be divided into partitions 
            . one partition is called the boot partition which contains the OS and device drivers 
            . windows system places its boot code in the first sector on the hard disk termed as (MBR)master boot record
            . MBR contains 
                * boot code
                * a table listing the partitions for the hard disk
                * a flag indicating which partition the system should boot from
            . booting is initiated by running code residing in the system's ROM that directs the system to read the boot code from the MBR
            . once the system identifies the boot partition
                * it reads the first sector from that partition(boot sector)
                * continues with the reminder of the boot process including load various subsystems and system services  
            <img src="images/10.7.bootingfromdiskinwindows.jpg" alt="" width="9%" height="9%">
    = bad blocks 
# swap-space management
    = swap space use 
    = swap space location 
    = swap space management 
# raid structure
    = improvement of reliability va redundancy 
    = improvement in performance via parallelism 
    = RAID levels 
    = selecting RAID level 
    = extensions 
    = problems with RAID 
# stable-storage implementation
# extra
    = one dimensional array, one row and multiple columns 
    = two dimensional array, multiple rows and multiple columns 
    = large switched fabric, fabric is a number of switching elements connected together or bunch of interconnected switches 
    = topology, is the arrangement of nodes and connections within a network either physical or logical 
    = boot ROM, is a type of ROM that is used for booting a computer system
<hr>
<b id="id_filesysteminterface_detail">file-system interface</b>
<img src="images/0.os-birds-eye-view.jpg" alt="" width="9%" height="9%">
file concept
access methods
directory and disk structure
file-system mounting
file sharing
protection
summary
<hr>
<b id="id_filesystemimplementation_detail">file-system implementation</b>
<img src="images/0.os-birds-eye-view.jpg" alt="" width="9%" height="9%">
file-system structure
file-system implementation
directory implementation
allocation methods
free-space management
efficiency and performance
recovery
nfs
example: the wafl file system
summary
<hr>
<b id="id_i/osystems_detail">i/o systems</b>
<img src="images/0.os-birds-eye-view.jpg" alt="" width="9%" height="9%">
overview
i/o hardware
application i/o interface
kernel i/o subsystem
transforming i/o requests to hardware operations
streams
performance
summary
<hr>
<b id="id_protection_detail">protection</b>
<img src="images/0.os-birds-eye-view.jpg" alt="" width="9%" height="9%">
goals of protection
principles of protection
domain of protection
access matrix
implementation of the access matrix
access control
revocation of access rights
capability-based systems
language-based protection
summary
<hr>
<b id="id_security_detail">security</b>
<img src="images/0.os-birds-eye-view.jpg" alt="" width="9%" height="9%">
the security problem
program threats
system and network threats
cryptography as a security tool
user authentication
implementing security defenses
fire walling to protect systems and networks
computer-security classifications
an example: windows 7
summary
<hr>
<b id="id_virtualmachines_detail">virtual machines</b>
<img src="images/0.os-birds-eye-view.jpg" alt="" width="9%" height="9%">
overview
history
benefits and features
building blocks
types of virtual machines and their implementations
virtualization and operating-system components
examples
summary
<hr>
<b id="id_distributedsystems_detail">distributed systems</b>
<img src="images/0.os-birds-eye-view.jpg" alt="" width="9%" height="9%">
advantages of distributed systems
types of network based operating systems
network structure
communication structure
communication protocols
an example: tcp/ip
robustness
design issues
distributed file systems
summary
</pre>
</body>
</html>